


import os
import time
import requests
import json
import logging
import re
import sys
import asyncio
import aiohttp
from datetime import datetime
from bs4 import BeautifulSoup
from lxml import etree
from Utilities.logging_config import setup_logging
from Utilities.DB_connection import connect_to_mongodb
from Utilities.config import configureation

# Load configuration
config_yml = configureation()
config_data = requests.get(config_yml['config_path'])
config_da = config_data.json()
config = config_da['propertySources'][0]['source']

# Setup logging
setup_logging()
logger = logging.getLogger(__name__)

# Base URLs
main_url = "https://gc.nh.gov/rsa/html/nhtoc.htm"
base_url = "https://gc.nh.gov/rsa/html/"

# Master directory for storing XML files
master_directory = "New_Hampshire_RSA_Titles"
os.makedirs(master_directory, exist_ok=True)

CHAPTER_TEXT = """<?xml version="1.0"?>

<body>  
<div class="WordSection1">"""

# Function to fetch content asynchronously
async def fetch_content(url):
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=180) as response:
                return await response.text()
        except Exception as e:
            logger.error(f"Failed to fetch {url}: {e}")
            return None

# Extract the file content from the section link
async def get_file(section_url):
    html_content = await fetch_content(section_url)
    if not html_content:
        return ""

    soup = BeautifulSoup(html_content, 'html.parser')
    if soup.find("div", {"id": "print"}):
        download_content = soup.find("div", {"id": "print"})
        for table in soup.find_all('table'):
            table.decompose()
        return str(download_content)
    return ""

# Fetch table of contents for all titles
async def get_title_dict():
    html_content = await fetch_content(main_url)
    if not html_content:
        return {}

    soup = BeautifulSoup(html_content, 'html.parser')
    title_links = soup.find_all('a', href=True)
    title_dict = {a.text.strip(): base_url + a['href'].replace("../", "NHTOC/") for a in title_links if 'NHTOC/' in a['href']}
    
    return title_dict

# Process and download titles and chapters
async def download_KS_input():
    title_dict = await get_title_dict()
    logger.info(f"Found {len(title_dict)} titles. Processing...")

    for title_name, title_url in title_dict.items():
        logger.info(f"\nüìñ Fetching {title_name} from {title_url}...")

        safe_title_name = title_name.replace(':', ' -').replace('/', '-').strip()
        title_directory = os.path.join(master_directory, safe_title_name)
        os.makedirs(title_directory, exist_ok=True)

        title_content = await fetch_content(title_url)
        if not title_content:
            logger.warning(f"‚ö†Ô∏è Skipping {title_name} - Failed to fetch title page.")
            continue

        title_soup = BeautifulSoup(title_content, 'html.parser')
        chapter_links = title_soup.find_all('a', href=True)
        chapter_dict = {a.text.strip(): base_url + "NHTOC/" + a['href'] for a in chapter_links if 'NHTOC-' in a['href']}

        logger.info(f"üìÇ Found {len(chapter_dict)} chapters in {title_name}. Processing...")

        for chapter_name, chapter_url in chapter_dict.items():
            logger.info(f"\nüìÑ Fetching {chapter_name} from {chapter_url}...")

            time.sleep(1)
            chapter_content = await fetch_content(chapter_url)
            if not chapter_content:
                logger.warning(f"‚ö†Ô∏è Skipping {chapter_name} - Failed to fetch chapter page.")
                continue

            chapter_soup = BeautifulSoup(chapter_content, 'html.parser')
            body_tag = chapter_soup.find('body')
            if not body_tag:
                logger.warning(f"‚ö†Ô∏è Skipping {chapter_name} - No <body> tag found.")
                continue

            section_link_tag = None
            for h2_tag in body_tag.find_all('h2'):
                a_tag = h2_tag.find('a', href=True)
                if a_tag and "../" in a_tag["href"]:
                    section_link_tag = a_tag
                    break

            if not section_link_tag:
                logger.warning(f"‚ö†Ô∏è Skipping {chapter_name} - No valid section link found.")
                continue

            section_url = base_url + section_link_tag['href'].replace('../', '')
            logger.info(f"üîó Extracted section link: {section_url}")

            section_html = await fetch_content(section_url)
            if not section_html:
                logger.warning(f"‚ö†Ô∏è Skipping {chapter_name} - Failed to fetch section page.")
                continue

            section_soup = BeautifulSoup(section_html, 'html.parser')
            meta_tag = section_soup.find('meta', {'name': 'chapter'})
            if meta_tag:
                chapter_name = meta_tag['content'].strip()
            safe_chapter_name = chapter_name.replace(':', ' -').replace('/', '-').strip()

            chapter_content = CHAPTER_TEXT
            chapter_content += f'\n<p class="c1">{title_name}</p>'
            chapter_content += f'\n<p class="c1">{chapter_name}</p>'

            sections = section_soup.find_all('center')
            seen_sections = set()
            section_count = 0

            for section in sections:
                section_title_tag = section.find_next('b')
                if section_title_tag:
                    section_text = section_title_tag.get_text(strip=True)

                    if section_text in seen_sections:
                        continue
                    seen_sections.add(section_text)

                    section_body = section.find_next('codesect')
                    section_content_text = section_body.get_text("\n").strip() if section_body else "No content available"

                    formatted_content = "\n".join([f"<h3>{line.strip()}</h3>" if re.match(r"^ÓÄÅ?[a-z0-9]+ÓÄÅ?\.", line.strip()) else f"<p>{line.strip()}</p>" for line in section_content_text.split("\n") if line.strip()])

                    chapter_content += f'\n<h1 class="c2">{section_text}</h1>'
                    chapter_content += formatted_content

                    source_note = section.find_next('sourcenote')
                    if source_note:
                        history_text = source_note.get_text(" ", strip=True)
                        chapter_content += f'\n<p class="gc.nh.gov"><span class="history"> History: </span> {history_text}</p>'

                    section_count += 1

            chapter_content += "\n</div>\n</body>"

            if section_count > 0:
                xml_filename = os.path.join(title_directory, f'{safe_chapter_name}.xml')
                with open(xml_filename, 'w', encoding='utf-8') as f:
                    f.write(chapter_content)

                logger.info(f"‚úÖ Saved {section_count} sections in {xml_filename}")
            else:
                logger.warning(f"‚ö†Ô∏è No sections found for {chapter_name}. Skipping.")

# Validate titles and chapters
async def validate_KS_input():
    done = await get_title_dict()
    if done:
        logger.info(f"Validation successful. Found {len(done)} titles.")
    else:
        logger.error("Validation failed. No titles found.")

# Fetch table of contents
async def toc_KS():
    toc_list = await get_title_dict()
    toc_dict = {"hierarchy": [{"chunk": {"label": key, "title": val}} for key, val in toc_list.items()]}
    return json.dumps(toc_dict, indent=4)

if __name__ == "__main__":
    asyncio.run(download_KS_input())










2nd code: 
import os import time import requests import json import logging import asyncio import aiohttp import re from datetime import datetime from bs4 import BeautifulSoup from Utilities.DB_connection import connect_to_mongodb from Utilities.config import configureation from Utilities.logging_config import setup_logging

Load configuration

config_yml = configureation() config_data = requests.get(config_yml['config_path']) config_da = config_data.json() config = config_da['propertySources'][0]['source']

Setup logging

setup_logging() logger = logging.getLogger(name)

Constants

main_url = "https://gc.nh.gov/rsa/html/nhtoc.htm" base_url = "https://gc.nh.gov/rsa/html/" CHAPTER_TEXT = """<?xml version="1.0"?>

<body>  
<div class="WordSection1">"""Create master directory

master_directory = "New_Hampshire_RSA_Titles" os.makedirs(master_directory, exist_ok=True)

async def get_content(url): async with aiohttp.ClientSession() as session: async with session.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=180) as response: return await response.text()

async def fetch_titles(): response_text = await get_content(main_url) soup = BeautifulSoup(response_text, 'html.parser') title_links = soup.find_all('a', href=True) return [(a.text.strip(), base_url + a['href'].replace("../", "NHTOC/")) for a in title_links if 'NHTOC/' in a['href']]

async def fetch_chapters(title_name, title_url): response_text = await get_content(title_url) soup = BeautifulSoup(response_text, 'html.parser') chapter_links = soup.find_all('a', href=True) return [(a.text.strip(), base_url + "NHTOC/" + a['href']) for a in chapter_links if 'NHTOC-' in a['href']]

async def fetch_sections(chapter_url): response_text = await get_content(chapter_url) soup = BeautifulSoup(response_text, 'html.parser') return soup

async def process_title(title_name, title_url): print(f"Fetching {title_name} from {title_url}...") safe_title_name = title_name.replace(':', ' -').replace('/', '-').strip() title_directory = os.path.join(master_directory, safe_title_name) os.makedirs(title_directory, exist_ok=True) chapters = await fetch_chapters(title_name, title_url) for chapter_name, chapter_url in chapters: await process_chapter(title_name, chapter_name, chapter_url, title_directory)

async def process_chapter(title_name, chapter_name, chapter_url, title_directory): print(f"Fetching {chapter_name} from {chapter_url}...") time.sleep(1) chapter_soup = await fetch_sections(chapter_url) body_tag = chapter_soup.find('body') if not body_tag: print(f"Skipping {chapter_name} - No <body> tag found.") return section_link_tag = None for h2_tag in body_tag.find_all('h2'): a_tag = h2_tag.find('a', href=True) if a_tag and "../" in a_tag["href"]: section_link_tag = a_tag break if not section_link_tag: print(f"Skipping {chapter_name} - No valid section link found.") return section_url = base_url + section_link_tag['href'].replace('../', '') print(f"Extracted section link: {section_url}") await process_sections(title_name, chapter_name, section_url, title_directory)

async def process_sections(title_name, chapter_name, section_url, title_directory): section_soup = await fetch_sections(section_url) meta_tag = section_soup.find('meta', {'name': 'chapter'}) if meta_tag: chapter_name = meta_tag['content'].strip() safe_chapter_name = chapter_name.replace(':', ' -').replace('/', '-').strip() chapter_content = CHAPTER_TEXT chapter_content += f'\n<p class="c1">{title_name}</p>' chapter_content += f'\n<p class="c1">{chapter_name}</p>' sections = section_soup.find_all('center') seen_sections = set() section_count = 0 for section in sections: section_title_tag = section.find_next('b') if section_title_tag: section_text = section_title_tag.get_text(strip=True) if section_text in seen_sections: continue seen_sections.add(section_text) section_content = section.find_next('codesect') section_content_text = section_content.get_text("\n").strip() if section_content else "No content available" formatted_content = "\n".join([f"<h3>{line.strip()}</h3>" if re.match(r"^?", line.strip()) else f"<p>{line.strip()}</p>" for line in section_content_text.split("\n") if line.strip()]) chapter_content += f'\n<h1 class="c2">{section_text}</h1>' chapter_content += formatted_content source_note = section.find_next('sourcenote') if source_note: history_text = source_note.get_text(" ", strip=True) chapter_content += f'\n<p class="gc.nh.gov"><span class="history"> History: </span> {history_text}</p>' section_count += 1 chapter_content += "\n</div>\n</body>" if section_count > 0: xml_filename = os.path.join(title_directory, f'{safe_chapter_name}.xml') with open(xml_filename, 'w', encoding='utf-8') as f: f.write(chapter_content) print(f"Saved {section_count} sections in {xml_filename}") else: print(f"No sections found for {chapter_name}. Skipping.")

async def main(): titles = await fetch_titles() for title_name, title_url in titles: await process_title(title_name, title_url)

if name == "main": asyncio.run(main())


