

import os
import time
import requests
from bs4 import BeautifulSoup
import re

CHAPTER_TEXT = """<?xml version="1.0"?>

<body>  
<div class="WordSection1">"""

# Base URLs
main_url = "https://gc.nh.gov/rsa/html/nhtoc.htm"
base_url = "https://gc.nh.gov/rsa/html/"

# Create master directory
master_directory = "New_Hampshire_RSA_Titles"
os.makedirs(master_directory, exist_ok=True)

# Fetch main page
headers = {'User-Agent': 'Mozilla/5.0'}
try:
    response = requests.get(main_url, headers=headers, timeout=10)
    response.raise_for_status()
except requests.exceptions.RequestException as e:
    print(f"‚ùå Failed to fetch main page: {e}")
    exit()

soup = BeautifulSoup(response.text, 'html.parser')

# Find all titles
title_links = soup.find_all('a', href=True)
title_urls = [(a.text.strip(), base_url + a['href'].replace("../", "NHTOC/")) for a in title_links if 'NHTOC/' in a['href']]

print(f"Found {len(title_urls)} titles. Processing...")

# Loop through each title
for title_name, title_url in title_urls:
    print(f"\nüìñ Fetching {title_name} from {title_url}...")

    # Create title directory
    safe_title_name = title_name.replace(':', ' -').replace('/', '-').strip()
    title_directory = os.path.join(master_directory, safe_title_name)
    os.makedirs(title_directory, exist_ok=True)

    # Fetch title page
    try:
        title_response = requests.get(title_url, headers=headers, timeout=10)
        title_response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"‚ö†Ô∏è Skipping {title_name} - Failed to fetch title page: {e}")
        continue

    title_soup = BeautifulSoup(title_response.text, 'html.parser')

    # Find all chapter links under this title
    chapter_links = title_soup.find_all('a', href=True)
    chapter_urls = list(set([(a.text.strip(), base_url + "NHTOC/" + a['href']) for a in chapter_links if 'NHTOC-' in a['href']]))

    print(f"üìÇ Found {len(chapter_urls)} chapters in {title_name}. Processing...")

    # Loop through each chapter
    for chapter_name, chapter_url in chapter_urls:
        print(f"\nüìÑ Fetching {chapter_name} from {chapter_url}...")

        time.sleep(1)  # Prevent overloading the server

        # Fetch chapter page
        try:
            chapter_response = requests.get(chapter_url, headers=headers, timeout=10)
            chapter_response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"‚ö†Ô∏è Skipping {chapter_name} - Failed to fetch chapter page: {e}")
            continue

        chapter_soup = BeautifulSoup(chapter_response.text, 'html.parser')

        # Ensure body exists
        body_tag = chapter_soup.find('body')
        if not body_tag:
            print(f"‚ö†Ô∏è Skipping {chapter_name} - No <body> tag found.")
            continue

        # Find section link inside <h2>
        section_link_tag = None
        for h2_tag in body_tag.find_all('h2'):
            a_tag = h2_tag.find('a', href=True)
            if a_tag and "../" in a_tag["href"]:
                section_link_tag = a_tag
                break

        if not section_link_tag:
            print(f"‚ö†Ô∏è Skipping {chapter_name} - No valid section link found.")
            continue

        section_url = base_url + section_link_tag['href'].replace('../', '')
        print(f"üîó Extracted section link: {section_url}")

        # Fetch sections page
        try:
            section_response = requests.get(section_url, headers=headers, timeout=10)
            section_response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"‚ö†Ô∏è Skipping {chapter_name} - Failed to fetch section page: {e}")
            continue

        section_soup = BeautifulSoup(section_response.text, 'html.parser')

        # Extract Chapter Name from metadata
        meta_tag = section_soup.find('meta', {'name': 'chapter'})
        if meta_tag:
            chapter_name = meta_tag['content'].strip()
        safe_chapter_name = chapter_name.replace(':', ' -').replace('/', '-').strip()

        # Create file content
        chapter_content = CHAPTER_TEXT  # Start XML file with the defined structure
        chapter_content += f'\n<p class="c1">{title_name}</p>'
        chapter_content += f'\n<p class="c1">{chapter_name}</p>'

        # Find all sections
        sections = section_soup.find_all('center')

        section_count = 0
        seen_sections = set()  # Avoid duplicates

        for section in sections:
            section_title = section.find('b')
            if section_title:
                section_text = section_title.get_text(strip=True)
                if section_text in seen_sections:
                    continue  # Avoid duplicate sections
                seen_sections.add(section_text)

                # Extract content
                section_content = section.find_next('codesect')
                section_content_text = section_content.get_text("\n", strip=True) if section_content else "No content available"

                # Split content into separate lines for pointers like (a), (b), (1), (2)
                formatted_content = ""
                for line in re.split(r'(ÓÄÅ\wÓÄÅ|\d+\.)', section_content_text):
                    if re.match(r'^ÓÄÅ\wÓÄÅ|\d+\.$', line.strip()):  # If it's a pointer (a), (b), (1), (2)
                        formatted_content += f'\n<h2>{line.strip()}</h2>'
                    else:
                        formatted_content += f'\n<h3>{line.strip()}</h3>'

                # Format as requested
                chapter_content += f'\n<h1 class="c2">{section_text}</h1>'
                chapter_content += formatted_content

                # Extract history if available
                source_note = section.find_next('sourcenote')
                if source_note:
                    history_text = source_note.get_text(strip=True)
                    chapter_content += f'\n<p class="gc.nh.gov"><span class="history"> History: </span> {history_text}</p>'

                section_count += 1

        chapter_content += "\n</div>\n</body>"

        if section_count > 0:
            # Save XML file
            xml_filename = os.path.join(title_directory, f'{safe_chapter_name}.xml')
            with open(xml_filename, 'w', encoding='utf-8') as f:
                f.write(chapter_content)

            print(f"‚úÖ Saved {section_count} sections in {xml_filename}")
        else:
            print(f"‚ö†Ô∏è No sections found for {chapter_name}. Skipping.")

print("\n‚úÖ All titles and chapters processed successfully!")
