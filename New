


import os
import time
import requests
from bs4 import BeautifulSoup
import re

CHAPTER_TEXT = """<?xml version="1.0"?>

<body>  
<div class="WordSection1">"""

# Base URLs
main_url = "https://gc.nh.gov/rsa/html/nhtoc.htm"
base_url = "https://gc.nh.gov/rsa/html/"

# Create master directory
master_directory = "New_Hampshire_RSA_Titles"
os.makedirs(master_directory, exist_ok=True)

# Fetch main page
headers = {'User-Agent': 'Mozilla/5.0'}
try:
    response = requests.get(main_url, headers=headers, timeout=10)
    response.raise_for_status()
except requests.exceptions.RequestException as e:
    print(f"‚ùå Failed to fetch main page: {e}")
    exit()

soup = BeautifulSoup(response.text, 'html.parser')

# Find all titles
title_links = soup.find_all('a', href=True)
title_urls = [(a.text.strip(), base_url + a['href'].replace("../", "NHTOC/")) for a in title_links if 'NHTOC/' in a['href']]

print(f"Found {len(title_urls)} titles. Processing...")

# Loop through each title
for title_name, title_url in title_urls:
    print(f"\nüìñ Fetching {title_name} from {title_url}...")

    # Create title directory
    safe_title_name = title_name.replace(':', ' -').replace('/', '-').strip()
    title_directory = os.path.join(master_directory, safe_title_name)
    os.makedirs(title_directory, exist_ok=True)

    # Fetch title page
    try:
        title_response = requests.get(title_url, headers=headers, timeout=10)
        title_response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"‚ö†Ô∏è Skipping {title_name} - Failed to fetch title page: {e}")
        continue

    title_soup = BeautifulSoup(title_response.text, 'html.parser')

    # Find all chapter links under this title
    chapter_links = title_soup.find_all('a', href=True)
    chapter_urls = [(a.text.strip(), base_url + "NHTOC/" + a['href']) for a in chapter_links if 'NHTOC-' in a['href']]

    print(f"üìÇ Found {len(chapter_urls)} chapters in {title_name}. Processing...")

    # Loop through each chapter
    for chapter_name, chapter_url in chapter_urls:
        print(f"\nüìÑ Fetching {chapter_name} from {chapter_url}...")

        time.sleep(1)  # Prevent overloading the server

        # Fetch chapter page
        try:
            chapter_response = requests.get(chapter_url, headers=headers, timeout=10)
            chapter_response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"‚ö†Ô∏è Skipping {chapter_name} - Failed to fetch chapter page: {e}")
            continue

        chapter_soup = BeautifulSoup(chapter_response.text, 'html.parser')

        # Ensure body exists
        body_tag = chapter_soup.find('body')
        if not body_tag:
            print(f"‚ö†Ô∏è Skipping {chapter_name} - No <body> tag found.")
            continue

        # Find section link inside <h2>
        section_link_tag = None
        for h2_tag in body_tag.find_all('h2'):
            a_tag = h2_tag.find('a', href=True)
            if a_tag and "../" in a_tag["href"]:
                section_link_tag = a_tag
                break

        if not section_link_tag:
            print(f"‚ö†Ô∏è No valid section link found for {chapter_name}. Checking for standalone content...")

            # Extract standalone chapter content
            standalone_content = body_tag.find_all(['h1', 'h2', 'p', 'b'])
            chapter_texts = [tag.get_text(" ", strip=True) for tag in standalone_content if tag.get_text(strip=True)]

            if chapter_texts:
                chapter_content = CHAPTER_TEXT
                chapter_content += f'\n<p class="c1">{title_name}</p>'
                chapter_content += f'\n<p class="c1">{chapter_name}</p>'

                for text in chapter_texts:
                    if "Repealed" in text:
                        chapter_content += f'\n<h1 class="c2">{text}</h1>'
                    else:
                        chapter_content += f'\n<p>{text}</p>'

                chapter_content += "\n</div>\n</body>"

                # Save XML file
                xml_filename = os.path.join(title_directory, f'{chapter_name.replace(":", " -").replace("/", "-").strip()}.xml')
                with open(xml_filename, 'w', encoding='utf-8') as f:
                    f.write(chapter_content)

                print(f"‚úÖ Saved standalone chapter content in {xml_filename}")
                continue
            else:
                print(f"‚ö†Ô∏è Skipping {chapter_name} - No sections or standalone content found.")
                continue

        section_url = base_url + section_link_tag['href'].replace('../', '')
        print(f"üîó Extracted section link: {section_url}")

        # Fetch sections page
        try:
            section_response = requests.get(section_url, headers=headers, timeout=10)
            section_response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"‚ö†Ô∏è Skipping {chapter_name} - Failed to fetch section page: {e}")
            continue

        section_soup = BeautifulSoup(section_response.text, 'html.parser')

        # Extract Chapter Name from metadata
        meta_tag = section_soup.find('meta', {'name': 'chapter'})
        if meta_tag:
            chapter_name = meta_tag['content'].strip()
        safe_chapter_name = chapter_name.replace(':', ' -').replace('/', '-').strip()

        # Create file content
        chapter_content = CHAPTER_TEXT
        chapter_content += f'\n<p class="c1">{title_name}</p>'
        chapter_content += f'\n<p class="c1">{chapter_name}</p>'

        # Find all sections
        sections = section_soup.find_all('center')

        seen_sections = set()  # Prevent duplicate sections
        section_count = 0

        for section in sections:
            section_title_tag = section.find_next('b')
            if section_title_tag:
                section_text = section_title_tag.get_text(strip=True)

                if section_text in seen_sections:
                    continue  # Skip duplicate sections
                seen_sections.add(section_text)

                # Extract section content
                section_content = section.find_next('codesect')
                section_content_text = section_content.get_text("\n").strip() if section_content else "No content available"

                # Format content
                chapter_content += f'\n<h1 class="c2">{section_text}</h1>'
                chapter_content += f'\n<p>{section_content_text}</p>'

                # Extract history if available
                source_note = section.find_next('sourcenote')
                if source_note:
                    history_text = source_note.get_text(" ", strip=True)
                    chapter_content += f'\n<p class="gc.nh.gov"><span class="history"> History: </span> {history_text}</p>'

                section_count += 1

        chapter_content += "\n</div>\n</body>"

        if section_count > 0:
            # Save XML file
            xml_filename = os.path.join(title_directory, f'{safe_chapter_name}.xml')
            with open(xml_filename, 'w', encoding='utf-8') as f:
                f.write(chapter_content)

            print(f"‚úÖ Saved {section_count} sections in {xml_filename}")
        else:
            print(f"‚ö†Ô∏è No sections found for {chapter_name}. Skipping.")

print("\n‚úÖ All titles and chapters processed successfully!")
