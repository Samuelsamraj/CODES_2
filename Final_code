


import aiohttp
import asyncio
import json
import logging
import os
import re
import sys
from datetime import datetime
from bs4 import BeautifulSoup
from lxml import etree
from Utilities.logging_config import setup_logging
from Utilities.DB_connection import connect_to_mongodb
from Utilities.config import configureation

# Load configuration
config_yml = configureation()
config_data = requests.get(config_yml['config_path'])
config_da = config_data.json()
config = config_da['propertySources'][0]['source']

# Setup logging
setup_logging()
logger = logging.getLogger(__name__)

# Constants
BASE_URL = "https://gc.nh.gov/rsa/html/"
MAIN_URL = "https://gc.nh.gov/rsa/html/nhtoc.htm"
MASTER_DIRECTORY = "New_Hampshire_RSA_Titles"
CHAPTER_TEXT = """<?xml version="1.0"?>
<body>  
<div class="WordSection1">"""

os.makedirs(MASTER_DIRECTORY, exist_ok=True)
HEADERS = {'User-Agent': 'Mozilla/5.0'}

async def get_content(url):
    """Fetch content asynchronously from a URL"""
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(url, headers=HEADERS, timeout=180) as response:
                response.raise_for_status()
                return BeautifulSoup(await response.text(), 'html.parser')
        except Exception as e:
            logger.error(f"Unable to fetch content from {url}: {e}")
            return None

async def get_title_dict():
    """Fetch title dictionary from the main page"""
    soup = await get_content(MAIN_URL)
    if not soup:
        return {}

    title_links = soup.find_all('a', href=True)
    return {
        a.text.strip(): BASE_URL + a['href'].replace("../", "NHTOC/")
        for a in title_links if 'NHTOC/' in a['href']
    }

async def fetch_title(title_name, title_url):
    """Fetch and process each title"""
    logger.info(f"Downloading title: {title_name}")

    safe_title_name = title_name.replace(':', ' -').replace('/', '-').strip()
    title_directory = os.path.join(MASTER_DIRECTORY, safe_title_name)
    os.makedirs(title_directory, exist_ok=True)

    title_soup = await get_content(title_url)
    if not title_soup:
        return {}

    chapter_links = title_soup.find_all('a', href=True)
    chapter_urls = {
        a.text.strip(): BASE_URL + "NHTOC/" + a['href']
        for a in chapter_links if 'NHTOC-' in a['href']
    }

    return {"title_name": title_name, "title_url": title_url, "chapters": chapter_urls, "directory": title_directory}

async def fetch_chapter(chapter_name, chapter_url, title_name, title_directory):
    """Fetch and process each chapter"""
    logger.info(f"Fetching chapter: {chapter_name}")
    chapter_soup = await get_content(chapter_url)
    if not chapter_soup:
        return None

    body_tag = chapter_soup.find('body')
    if not body_tag:
        logger.warning(f"Skipping {chapter_name} - No <body> tag found.")
        return None

    section_link_tag = next(
        (a_tag for h2_tag in body_tag.find_all('h2') if (a_tag := h2_tag.find('a', href=True)) and "../" in a_tag["href"]), 
        None
    )

    if not section_link_tag:
        logger.warning(f"Skipping {chapter_name} - No valid section link found.")
        return None

    section_url = BASE_URL + section_link_tag['href'].replace('../', '')
    return await fetch_section(section_url, chapter_name, title_name, title_directory)

async def fetch_section(section_url, chapter_name, title_name, title_directory):
    """Fetch section data and return formatted XML"""
    section_soup = await get_content(section_url)
    if not section_soup:
        return None

    meta_tag = section_soup.find('meta', {'name': 'chapter'})
    if meta_tag:
        chapter_name = meta_tag['content'].strip()
    safe_chapter_name = chapter_name.replace(':', ' -').replace('/', '-').strip()

    chapter_content = CHAPTER_TEXT
    chapter_content += f'\n<p class="c1">{title_name}</p>'
    chapter_content += f'\n<p class="c1">{chapter_name}</p>'

    sections = section_soup.find_all('center')
    seen_sections = set()
    section_count = 0

    for section in sections:
        section_title_tag = section.find_next('b')
        if section_title_tag:
            section_text = section_title_tag.get_text(strip=True)

            if section_text in seen_sections:
                continue
            seen_sections.add(section_text)

            section_content = section.find_next('codesect')
            section_content_text = section_content.get_text("\n").strip() if section_content else "No content available"

            formatted_content = "\n".join([
                f"<h3>{line.strip()}</h3>" if re.match(r"^?[a-z0-9]+?\.", line.strip()) else f"<p>{line.strip()}</p>"
                for line in section_content_text.split("\n") if line.strip()
            ])

            chapter_content += f'\n<h1 class="c2">{section_text}</h1>'
            chapter_content += formatted_content

            source_note = section.find_next('sourcenote')
            if source_note:
                history_text = source_note.get_text(" ", strip=True)
                chapter_content += f'\n<p class="gc.nh.gov"><span class="history"> History: </span> {history_text}</p>'

            section_count += 1

    chapter_content += "\n</div>\n</body>"

    if section_count > 0:
        xml_filename = os.path.join(title_directory, f'{safe_chapter_name}.xml')
        with open(xml_filename, 'w', encoding='utf-8') as f:
            f.write(chapter_content)
        logger.info(f"Saved {section_count} sections in {xml_filename}")
        return xml_filename
    else:
        logger.warning(f"No sections found for {chapter_name}. Skipping.")
        return None

async def download_NH_input():
    """Expanded download function"""
    toc_list = await get_title_dict()
    for title_name, title_url in toc_list.items():
        title_data = await fetch_title(title_name, title_url)
        for chapter_name, chapter_url in title_data["chapters"].items():
            await fetch_chapter(chapter_name, chapter_url, title_data["title_name"], title_data["directory"])

async def validate_NH_input():
    """Expanded validation function with MongoDB"""
    with connect_to_mongodb() as client:
        logger.info(f"Connected to {config['docdb']} database")
        db = client[config['docdb']]
        collection = db[config['collection_name']]
        titles = await get_title_dict()
        done, wip = [], []

        for title, url in titles.items():
            if collection.find_one({"jx": 'NH', "titleNo": title}):
                done.append({"title": title})
            else:
                wip.append({"title": title})
                asyncio.create_task(download_NH_input())

        return done, wip

async def toc_NH():
    """Expanded TOC function"""
    toc_list = await get_title_dict()
    root = etree.Element("hierarchy")
    
    for key, val in toc_list.items():
        chunk = etree.SubElement(root, "chunk", label=key, title=val)

    return etree.tostring(root, encoding='unicode')

async def main():
    """Main function"""
    await download_NH_input()
    await validate_NH_input()
    toc_xml = await toc_NH()
    logger.info("TOC Generated")
    print(toc_xml)

if __name__ == "__main__":
    asyncio.run(main())
