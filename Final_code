


import aiohttp
import asyncio
import json
import logging
import os
import re
import sys
from datetime import datetime
from bs4 import BeautifulSoup
from lxml import etree
from Utilities.logging_config import setup_logging
from Utilities.DB_connection import connect_to_mongodb
from Utilities.config import configureation

# Load configuration
config_yml = configureation()
config_data = requests.get(config_yml['config_path'])
config_da = config_data.json()
config = config_da['propertySources'][0]['source']

# Setup logging
setup_logging()
logger = logging.getLogger(__name__)

# Constants
BASE_URL = "https://gc.nh.gov/rsa/html/"
MAIN_URL = "https://gc.nh.gov/rsa/html/nhtoc.htm"
MASTER_DIRECTORY = "New_Hampshire_RSA_Titles"
CHAPTER_TEXT = """<?xml version="1.0"?>
<body>  
<div class="WordSection1">"""

os.makedirs(MASTER_DIRECTORY, exist_ok=True)
HEADERS = {'User-Agent': 'Mozilla/5.0'}

async def get_content(url):
    """Fetch content asynchronously from a URL"""
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(url, headers=HEADERS, timeout=180) as response:
                response.raise_for_status()
                return BeautifulSoup(await response.text(), 'html.parser')
        except Exception as e:
            logger.error(f"Unable to fetch content from {url}: {e}")
            return None

async def get_title_dict():
    """Fetch title dictionary from the main page"""
    soup = await get_content(MAIN_URL)
    if not soup:
        return {}

    title_links = soup.find_all('a', href=True)
    return {
        a.text.strip(): BASE_URL + a['href'].replace("../", "NHTOC/")
        for a in title_links if 'NHTOC/' in a['href']
    }

async def fetch_title(title_name, title_url):
    """Fetch and process each title"""
    logger.info(f"Downloading title: {title_name}")

    safe_title_name = title_name.replace(':', ' -').replace('/', '-').strip()
    title_directory = os.path.join(MASTER_DIRECTORY, safe_title_name)
    os.makedirs(title_directory, exist_ok=True)

    title_soup = await get_content(title_url)
    if not title_soup:
        return {}

    chapter_links = title_soup.find_all('a', href=True)
    chapter_urls = {
        a.text.strip(): BASE_URL + "NHTOC/" + a['href']
        for a in chapter_links if 'NHTOC-' in a['href']
    }

    return {"title_name": title_name, "title_url": title_url, "chapters": chapter_urls, "directory": title_directory}

async def fetch_chapter(chapter_name, chapter_url, title_name, title_directory):
    """Fetch and process each chapter"""
    logger.info(f"Fetching chapter: {chapter_name}")
    chapter_soup = await get_content(chapter_url)
    if not chapter_soup:
        return None

    body_tag = chapter_soup.find('body')
    if not body_tag:
        logger.warning(f"Skipping {chapter_name} - No <body> tag found.")
        return None

    section_link_tag = next(
        (a_tag for h2_tag in body_tag.find_all('h2') if (a_tag := h2_tag.find('a', href=True)) and "../" in a_tag["href"]), 
        None
    )

    if not section_link_tag:
        logger.warning(f"Skipping {chapter_name} - No valid section link found.")
        return None

    section_url = BASE_URL + section_link_tag['href'].replace('../', '')
    return await fetch_section(section_url, chapter_name, title_name, title_directory)

async def fetch_section(section_url, chapter_name, title_name, title_directory):
    """Fetch section data and return formatted XML"""
    section_soup = await get_content(section_url)
    if not section_soup:
        return None

    meta_tag = section_soup.find('meta', {'name': 'chapter'})
    if meta_tag:
        chapter_name = meta_tag['content'].strip()
    safe_chapter_name = chapter_name.replace(':', ' -').replace('/', '-').strip()

    chapter_content = CHAPTER_TEXT
    chapter_content += f'\n<p class="c1">{title_name}</p>'
    chapter_content += f'\n<p class="c1">{chapter_name}</p>'

    sections = section_soup.find_all('center')
    seen_sections = set()
    section_count = 0

    for section in sections:
        section_title_tag = section.find_next('b')
        if section_title_tag:
            section_text = section_title_tag.get_text(strip=True)

            if section_text in seen_sections:
                continue
            seen_sections.add(section_text)

            section_content = section.find_next('codesect')
            section_content_text = section_content.get_text("\n").strip() if section_content else "No content available"

            formatted_content = "\n".join([
                f"<h3>{line.strip()}</h3>" if re.match(r"^?[a-z0-9]+?\.", line.strip()) else f"<p>{line.strip()}</p>"
                for line in section_content_text.split("\n") if line.strip()
            ])

            chapter_content += f'\n<h1 class="c2">{section_text}</h1>'
            chapter_content += formatted_content

            source_note = section.find_next('sourcenote')
            if source_note:
                history_text = source_note.get_text(" ", strip=True)
                chapter_content += f'\n<p class="gc.nh.gov"><span class="history"> History: </span> {history_text}</p>'

            section_count += 1

    chapter_content += "\n</div>\n</body>"

    if section_count > 0:
        xml_filename = os.path.join(title_directory, f'{safe_chapter_name}.xml')
        with open(xml_filename, 'w', encoding='utf-8') as f:
            f.write(chapter_content)
        logger.info(f"Saved {section_count} sections in {xml_filename}")
        return xml_filename
    else:
        logger.warning(f"No sections found for {chapter_name}. Skipping.")
        return None

async def download_NH_input():
    """Expanded download function"""
    toc_list = await get_title_dict()
    for title_name, title_url in toc_list.items():
        title_data = await fetch_title(title_name, title_url)
        for chapter_name, chapter_url in title_data["chapters"].items():
            await fetch_chapter(chapter_name, chapter_url, title_data["title_name"], title_data["directory"])

async def validate_NH_input():
    """Expanded validation function with MongoDB"""
    with connect_to_mongodb() as client:
        logger.info(f"Connected to {config['docdb']} database")
        db = client[config['docdb']]
        collection = db[config['collection_name']]
        titles = await get_title_dict()
        done, wip = [], []

        for title, url in titles.items():
            if collection.find_one({"jx": 'NH', "titleNo": title}):
                done.append({"title": title})
            else:
                wip.append({"title": title})
                asyncio.create_task(download_NH_input())

        return done, wip

async def toc_NH():
    """Expanded TOC function"""
    toc_list = await get_title_dict()
    root = etree.Element("hierarchy")
    
    for key, val in toc_list.items():
        chunk = etree.SubElement(root, "chunk", label=key, title=val)

    return etree.tostring(root, encoding='unicode')

async def main():
    """Main function"""
    await download_NH_input()
    await validate_NH_input()
    toc_xml = await toc_NH()
    logger.info("TOC Generated")
    print(toc_xml)

if __name__ == "__main__":
    asyncio.run(main())









async def download_NH_input(url, base_url, title, download, collection_path):
    """Download and process NH legal statutes by title"""
    
    collection_path = os.path.join(collection_path, f"Title_{title}")
    if not os.path.exists(collection_path):
        os.makedirs(collection_path)

    toc_list = await get_title_dict(url, collection_path)

    if title:
        title_url = re.sub(r'nhtoc\.htm', r'NHTOC/nhtoc_ch', url)
        title_url = title_url + title + ".htm"

        ack = await fetch_title(title_url, title, toc_list, collection_path)

        with connect_to_mongodb() as client:
            logger.info(f"Connected to {config['docdb']} database")
            db = client[config['docdb']]
            collection = db[config['collection_name']]

            if (book := collection.find_one({"jx": 'NH', "titleNo": title, "isDeleted": {"$exists": False}})) is not None:
                task_id = book["taskId"]
                query = {"taskId": task_id}
                file_details = []

                for file in ack.keys():
                    details = {
                        "Name": f"{title}_{file}.xml",
                        "collection": "true",
                        "common_conversion": "",
                        "mncr_conversion": "",
                        "meta": "",
                        "toc": "",
                        "type": config['NH_seclevel'],
                        "titleName": ack[file],
                        "normCite": ""
                    }
                    file_details.append(details)

                con_values = {"$set": {"fileDetail": file_details}}
                collection.update_one(query, con_values)

                total_file = len(ack)
                total = {"$set": {"totalFiles": total_file, "finalStage": "Conversion"}}
                collection.update_one(query, total)

                logger.info(f"Title {title} inserted into {config['collection_name']} collection")

            client.close()
            logger.info(f"Title {title} Conversion in progress")

            conv_url = config_yml['convert_request']
            payload = json.dumps({
                "data": {
                    "createdBy": "",
                    "createdDate": datetime.now().strftime("%Y-%m-%d"),
                    "firstleveno": f"{title}",
                    "jx": "NH",
                    "firstLeveltitle": ""
                },
                "additionalParam": {
                    "convRawXm1": "yes",
                    "convMncrXm1": "string"
                }
            }, indent=4)

            headers = {'Content-Type': 'application/json'}
            logger.info(f"Payload {payload}")

            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(conv_url, headers=headers, data=payload) as response:
                        request_resp = await response.text()
                        logger.info(f"Conversion request sent for {title}")
            except Exception as e:
                logger.error(f"Conversion request failed for {title} {e}")

        return ack
    else:
        for title_num, div_value in toc_list.items():
            title_url = re.sub(r'nhtoc\.htm', r'NHTOC/nhtoc_ch', url)
            title_url = title_url + title_num + ".htm"
            message = await fetch_title(title_url, title_num, toc_list, collection_path)

        return message






async def validate_NH_input(url, base_url, download, collection_path):
    """Validate NH legal statutes before processing"""

    with connect_to_mongodb() as client:
        logger.info(f"Connected to {config['docdb']} database")
        db = client[config['docdb']]
        collection = db[config['collection_name']]
        done = []
        wip = []

        for tn in download.download:
            title = tn.firstLeveno
            doc_title = tn.firstLeveltitle

            if (book := collection.find_one({"jx": 'NH', "titleNo": title, "isDeleted": {"$exists": False}})) is not None:
                downloaded_titles = {
                    "title": title,
                    "downloadDate": book["downloadDate"]
                }
                done.append(downloaded_titles)
            else:
                wip_titles = {
                    "Num": title,
                    "Title": doc_title,
                    "jX": download.jx,
                    "createdBy": download.createdBy,
                    "createdDate": download.createdDate
                }
                wip.append(wip_titles)

        for download in wip:
            title = download['Num']
            asyncio.create_task(download_NH_input(url, base_url, title, download, collection_path))

        query = {"taskId": {"$regex": "^NH"}}
        task_count = collection.count_documents(query) + 1
        tcount = task_count
        task_id = ""

        for tas in range(0, task_count):
            task_id = 'NH' + str(tcount)
            if (book := collection.find_one({"jx": 'NH', "taskId": task_id})) is not None:
                tcount += 1
            else:
                break

        current_date = datetime.now().date()
        formatted_date = current_date.strftime("%Y-%m-%d")
        create_date = download['createdDate']

        if isinstance(create_date, date):
            create_date = create_date.isoformat()

        file_details = []
        title_details = {
            "jx": download['jX'],
            "createdBy": download['createdBy'],
            "createdDate": create_date,
            "titleNo": title,
            "titleName": download['Title'],
            "downloadDate": formatted_date,
            "totalFiles": 0,
            "fileDetail": file_details,
            "finalStage": "Collection",
            "finalStatus": "In Progress",
            "taskId": task_id,
            "stateToc": "",
            "Conversion": "",
            "progress": ""
        }
        collection.insert_one(title_details)
        logger.info(f"Title {title} inserted into {config['collection_name']} collection")

    return done, wip
