


import aiohttp
import os
import json
import logging
import asyncio
import nest_asyncio
import requests
import re
import xml.etree.ElementTree as ET
from datetime import datetime, date
from bs4 import BeautifulSoup
from fastapi import FastAPI
from pymongo import MongoClient
from Utilities.config import configureation
from Utilities.logging_config import setup_logging
from Utilities.DB_connection import connect_to_mongodb

# Setup logging
setup_logging()
logger = logging.getLogger(__name__)

# Load configuration
config_yml = configureation()
config_data = requests.get(config_yml['config_path'])
config_da = config_data.json()
config = config_da['propertySources'][0]['source']

# Apply nested event loop
nest_asyncio.apply()

# Constants
BASE_URL = "https://gc.nh.gov/rsa/html/"
MAIN_URL = "https://gc.nh.gov/rsa/html/nhtoc.htm"
HEADERS = {'User-Agent': 'Mozilla/5.0'}

# Standard XML Header for Chapters
CHAPTER_TEXT = """<?xml version="1.0"?>
<body><div class="WordSection1">"""

# Function to fetch content from a given URL using aiohttp
async def get_content(url):
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=HEADERS, timeout=180) as response:
                response.raise_for_status()
                return BeautifulSoup(await response.text(), 'html.parser')
    except Exception as e:
        logger.error(f"Unable to fetch content from {url}: {e}")
        return None

# Fetch and parse the Table of Contents (TOC)
async def get_title_dict():
    soup = await get_content(MAIN_URL)
    if not soup:
        return {}

    title_links = soup.find_all('a', href=True)
    return {
        a.text.strip(): BASE_URL + a['href'].replace("../", "NHTOC/")
        for a in title_links if 'NHTOC/' in a['href']
    }

# Fetch and process each title
async def fetch_title(title_name, title_url):
    logger.info(f"Downloading title: {title_name}")

    title_soup = await get_content(title_url)
    if not title_soup:
        return {}

    chapter_links = title_soup.find_all('a', href=True)
    chapter_urls = {
        a.text.strip(): BASE_URL + "NHTOC/" + a['href']
        for a in chapter_links if 'NHTOC-' in a['href']
    }

    return {"title_name": title_name, "title_url": title_url, "chapters": chapter_urls}

# Fetch and process each chapter
async def fetch_chapter(chapter_name, chapter_url, title_name):
    logger.info(f"Fetching chapter: {chapter_name}")

    chapter_soup = await get_content(chapter_url)
    if not chapter_soup:
        return None

    body_tag = chapter_soup.find('body')
    if not body_tag:
        logger.warning(f"Skipping {chapter_name} - No <body> tag found.")
        return None

    section_link_tag = next(
        (a_tag for h2_tag in body_tag.find_all('h2') if (a_tag := h2_tag.find('a', href=True)) and "../" in a_tag["href"]), 
        None
    )

    if not section_link_tag:
        logger.warning(f"Skipping {chapter_name} - No valid section link found.")
        return None

    section_url = BASE_URL + section_link_tag['href'].replace('../', '')
    return await fetch_section(section_url, chapter_name, title_name)

# Fetch section data for storage
async def fetch_section(section_url, chapter_name, title_name):
    section_soup = await get_content(section_url)
    if not section_soup:
        return None

    meta_tag = section_soup.find('meta', {'name': 'chapter'})
    if meta_tag:
        chapter_name = meta_tag['content'].strip()

    chapter_content = {
        "title_name": title_name,
        "chapter_name": chapter_name,
        "sections": []
    }

    sections = section_soup.find_all('center')
    seen_sections = set()

    for section in sections:
        section_title_tag = section.find_next('b')
        if section_title_tag:
            section_text = section_title_tag.get_text(strip=True)

            if section_text in seen_sections:
                continue
            seen_sections.add(section_text)

            section_content = section.find_next('codesect')
            section_content_text = section_content.get_text("\n").strip() if section_content else "No content available"

            formatted_content = [
                {"type": "h3", "text": line.strip()} if re.match(r"^?[a-z0-9]+?\.", line.strip()) else {"type": "p", "text": line.strip()}
                for line in section_content_text.split("\n") if line.strip()
            ]

            chapter_content["sections"].append({
                "section_title": section_text,
                "content": formatted_content
            })

    return chapter_content if chapter_content["sections"] else None

# Download and process NH legal statutes
async def download_NH_input(url, base_url, title, download, Collection_path):
    Collection_path = os.path.join(Collection_path, f"Title_{title}")
    if not os.path.exists(Collection_path):
        os.makedirs(Collection_path)

    toc_list = await get_title_dict()

    if title:
        title_url = f"{BASE_URL}NHTOC/nhtoc_ch{title}.htm"
        ack = await fetch_title(title, title_url)

        with connect_to_mongodb() as client:
            logging.info(f"Connected to {config['docdb']} database")
            db = client[config['docdb']]
            collection = db[config['collection_name']]

            if (book := collection.find_one({"jx": 'NH', "titleNo": title, "isDeleted": {"$exists": False}})) is not None:
                task_id = book["taskId"]
                query = {"taskId": task_id}
                file_details = []

                for chapter_name, chapter_url in ack["chapters"].items():
                    chapter_data = await fetch_chapter(chapter_name, chapter_url, ack["title_name"])
                    
                    if chapter_data:
                        filename = f"{title}_{chapter_name}.xml"
                        details = {
                            "Name": filename,
                            "collection": "true",
                            "common_conversion": "",
                            "mncr_conversion": "",
                            "meta": "",
                            "toc": "",
                            "type": config['NH_seclevel'],
                            "titleName": chapter_name,
                            "normCite": ""
                        }
                        file_details.append(details)

                        file_path = os.path.join(Collection_path, filename)
                        with open(file_path, "w", encoding="utf-8") as file:
                            json.dump(chapter_data, file, indent=4)

                collection.update_one(query, {"$set": {"fileDetail": file_details}})
                collection.update_one(query, {"$set": {"totalFiles": len(file_details), "finalStage": "Conversion"}})

                logging.info(f"Title {title} inserted into {config['collection_name']} collection")

            client.close()
            logging.info(f"Title {title} Conversion in progress")

    else:
        for title_num in toc_list.keys():
            title_url = f"{BASE_URL}NHTOC/nhtoc_ch{title_num}.htm"
            message = await fetch_title(title_num, title_url)
        return message

# Generate TOC for NH
async def toc_NH():
    toc_list = await get_title_dict()
    root = ET.Element("hierarchy")

    for key, val in toc_list.items():
        title_number = key.split(':')[0].strip()
        title_name = key.split(':')[1].strip()

        child = ET.Element("chunk")
        child.set("label", title_number)
        child.set("title", title_name)
        root.append(child)

    return ET.tostring(root, encoding="unicode")
