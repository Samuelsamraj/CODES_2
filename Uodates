


import os
import aiohttp
import asyncio
import nest_asyncio
import json
import logging
import re
import xml.etree.ElementTree as ET
from datetime import datetime, date
from bs4 import BeautifulSoup
from pyppeteer import launch
from Utilities.logging_config import setup_logging
from Utilities.DB_connection import connect_to_mongodb
from Utilities.config import configureation

# Load configuration
config = configureation()
nest_asyncio.apply()

# Setup logging
setup_logging()
logger = logging.getLogger(__name__)

HEADERS = {'User-Agent': 'Mozilla/5.0'}

async def get_content(url):
    """Fetch content asynchronously using Pyppeteer (handles JavaScript rendering)"""
    try:
        browser = await launch(headless=True, args=['--no-sandbox'])
        page = await browser.newPage()
        await page.goto(url, {'waitUntil': 'networkidle2'})
        content = await page.content()
        await browser.close()
        return BeautifulSoup(content, 'html.parser')
    except Exception as e:
        logger.error(f"Failed to fetch content from {url}: {e}")
        return None

async def get_title_dict(url):
    """Fetch title dictionary from the main page dynamically"""
    soup = await get_content(url)
    if not soup:
        return {}

    title_links = soup.find_all('a', href=True)
    return {
        a.text.strip(): url + a['href'].replace("../", "NHTOC/")
        for a in title_links if 'NHTOC/' in a['href']
    }

async def fetch_title(title_name, title_url, base_url):
    """Fetch and process each title"""
    logger.info(f"Downloading title: {title_name}")
    title_soup = await get_content(title_url)
    if not title_soup:
        return {}

    chapter_links = title_soup.find_all('a', href=True)
    chapter_urls = {
        a.text.strip(): base_url + "NHTOC/" + a['href']
        for a in chapter_links if 'NHTOC-' in a['href']
    }

    return {"title_name": title_name, "title_url": title_url, "chapters": chapter_urls}

async def fetch_chapter(chapter_name, chapter_url, title_name):
    """Fetch and process each chapter"""
    logger.info(f"Fetching chapter: {chapter_name}")
    chapter_soup = await get_content(chapter_url)
    if not chapter_soup:
        return None

    body_tag = chapter_soup.find('body')
    if not body_tag:
        logger.warning(f"Skipping {chapter_name} - No <body> tag found.")
        return None

    section_link_tag = next(
        (a_tag for h2_tag in body_tag.find_all('h2') if (a_tag := h2_tag.find('a', href=True)) and "../" in a_tag["href"]), 
        None
    )

    if not section_link_tag:
        logger.warning(f"Skipping {chapter_name} - No valid section link found.")
        return None

    section_url = section_link_tag['href'].replace('../', '')
    return await fetch_section(section_url, chapter_name, title_name)

async def fetch_section(section_url, chapter_name, title_name):
    """Fetch section data and return formatted content for MongoDB storage"""
    section_soup = await get_content(section_url)
    if not section_soup:
        return None

    meta_tag = section_soup.find('meta', {'name': 'chapter'})
    if meta_tag:
        chapter_name = meta_tag['content'].strip()

    chapter_content = {
        "title_name": title_name,
        "chapter_name": chapter_name,
        "sections": []
    }

    sections = section_soup.find_all('center')
    seen_sections = set()

    for section in sections:
        section_title_tag = section.find_next('b')
        if section_title_tag:
            section_text = section_title_tag.get_text(strip=True)

            if section_text in seen_sections:
                continue
            seen_sections.add(section_text)

            section_content = section.find_next('codesect')
            section_content_text = section_content.get_text("\n").strip() if section_content else "No content available"

            formatted_content = [
                {"type": "h3", "text": line.strip()} if re.match(r"^?[a-z0-9]+?\.", line.strip()) else {"type": "p", "text": line.strip()}
                for line in section_content_text.split("\n") if line.strip()
            ]

            chapter_content["sections"].append({
                "section_title": section_text,
                "content": formatted_content
            })

    return chapter_content if chapter_content["sections"] else None

async def download_NH_input(url, base_url, title, download, collection_path):
    """Download and process NH legal statutes and store in MongoDB"""

    collection_path = os.path.join(collection_path, f"Title_{title}")
    if not os.path.exists(collection_path):
        os.makedirs(collection_path)

    toc_list = await get_title_dict(url)

    if title:
        title_url = f"{base_url}NHTOC/nhtoc_ch{title}.htm"
        ack = await fetch_title(title, title_url, base_url)

        with connect_to_mongodb() as client:
            logging.info(f"Connected to {config['docdb']} database")
            db = client[config['docdb']]
            collection = db[config['collection_name']]

            if (book := collection.find_one({"jx": 'NH', "titleNo": title, "isDeleted": {"$exists": False}})) is not None:
                task_id = book["taskId"]
                query = {"taskId": task_id}
                file_details = []

                for chapter_name, chapter_url in ack["chapters"].items():
                    chapter_data = await fetch_chapter(chapter_name, chapter_url, ack["title_name"])
                    
                    if chapter_data:
                        filename = f"title_{chapter_name}.xml"

                        details = {
                            "Name": filename,
                            "collection": "true",
                            "common_conversion": "",
                            "mncr_conversion": "",
                            "meta": "",
                            "toc": "",
                            "type": config['NH_seclevel'],
                            "titleName": chapter_name,
                            "normCite": ""
                        }
                        file_details.append(details)

                        file_path = os.path.join(collection_path, filename)
                        with open(file_path, "w", encoding="utf-8") as file:
                            json.dump(chapter_data, file, indent=4)

                collection.update_one(query, {"$set": {"fileDetail": file_details}})
                collection.update_one(query, {"$set": {"totalFiles": len(file_details), "finalStage": "Conversion"}})

                logging.info(f"Title {title} inserted into {config['collection_name']} collection")

            client.close()
            logging.info(f"Title {title} Conversion in progress")

async def toc_NH(url):
    toc_list = await get_title_dict(url)
    root = ET.Element("hierarchy")

    for key, val in toc_list.items():
        title_number = key.split(':')[0].strip().replace("TITLE", "").strip()
        title_name = key.split(':')[1].strip()

        child = ET.Element("chunk")
        child.set("label", title_number)
        child.set("title", title_name)
        root.append(child)

    return ET.tostring(root, encoding="unicode")
