





async def fetch_section(section_url, chapter_name, title_name, Collection_path):
    """Fetch section data and return formatted XML."""
    section_soup = await get_content(section_url)
    if not section_soup:
        return None, None

    # Extract title name and chapter name from meta tags
    meta_tag_2 = section_soup.find('meta', {'name': 'title'})
    if meta_tag_2:
        title_name = meta_tag_2['content'].strip()
    safe_title_name = title_name.replace(':', '.')

    meta_tag = section_soup.find('meta', {'name': 'chapter'})
    if meta_tag:
        chapter_name = meta_tag['content'].strip()

    # Replace ": " with " -", and sanitize for the chapter name
    safe_chapter_name = chapter_name.replace(':', '-').replace('/', '-').strip()

    chapter_content = CHAPTER_TEXT
    dl_counter = 1  # Initialize the counter for dl

    # Add title_name with dl
    chapter_content += f'\n<p class="c1" dl="{dl_counter}">{safe_title_name}</p>'
    dl_counter += 1  # Increment dl counter

    # Add chapter_name with dl
    chapter_content += f'\n<p class="c1" dl="{dl_counter}">{safe_chapter_name}</p>'
    dl_counter += 1  # Increment dl counter

    # Add section title without dl to <h1> tags
    dl_counter += 1  # Increment dl counter for the section title

    sections = section_soup.find_all('center')
    seen_sections = set()
    section_count = 0
    formatted_content = ""

    for section in sections:
        section_title_tag = section.find_next('b')
        if section_title_tag:
            section_text = section_title_tag.get_text(strip=True)
            section_text = section_text.replace(" -", "")  # Remove " -"

            # Replace colon with a dot in section title (for <h1> tags)
            section_text = section_text.replace(":", ".")

            if section_text in seen_sections:
                continue
            seen_sections.add(section_text)

            section_content = section.find_next('codesect')
            section_content_text = section_content.get_text("\n").strip() if section_content else "No content available"

            # Add section title without dl to <h1> tags
            chapter_content += f'\n<h1 class="c2">{section_text}</h1>'

            dl_counter += 1  # Increment dl_counter for the section title

            # Format section content
            lines = section_content_text.split("\n")
            current_article = None
            current_sub_item = None
            current_numbered_sub_item = None

            for line in lines:
                line = line.strip()
                if line:
                    # Match Article patterns like "Article I", "Article II", etc.
                    article_match = re.match(r'^(Article [IVXLCDM]+)', line)
                    if article_match:
                        current_article = article_match.group(1)
                        formatted_content += f'\n<p>{current_article}</p>'
                        dl_counter += 1  # Increment dl_counter for the article title

                    # Match sections like "I.", "II.", "III." etc.
                    elif re.match(r'^[A-Z]+\.$', line):
                        current_sub_item = line
                        formatted_content += f'\n<h2>{current_sub_item}</h2>'

                    elif re.match(r'^[A-Z]+\.\s.*', line):
                        current_sub_item_with_content = line
                        formatted_content += f'\n<h2>{current_sub_item_with_content}</h2>'

                    # Match subsections like "(a)", "(b)", "(c)" etc.
                    elif re.match(r'^[a-zA-Z]+', line):
                        current_numbered_sub_item = line
                        formatted_content += f'\n<h3>{current_numbered_sub_item} {line[1:].strip()}</h3>'

                    # Match numbered items like "1.", "2.", "3." etc.
                    elif re.match(r'^\d+\.', line):
                        formatted_content += f'\n<h4>{line}</h4>'

                    elif re.match(r'^\d+\s?', line):
                        formatted_content += f'\n<h4>{line}</h4>'

                    else:
                        formatted_content += f'\n<p>{line}</p>'

    # Merge <h2> tags with Roman numerals with the following <p> tag if they contain only Roman numerals
    soup = BeautifulSoup(formatted_content, 'html.parser')
    roman_pattern = re.compile(r'^\s*(M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3}))\s*\.?\s*$')

    h2_tags = soup.find_all('h2')

    for h2 in h2_tags:
        # Check if <h2> contains only a Roman numeral using regex
        if roman_pattern.match(h2.get_text()):
            # Find the next <p> tag sibling
            next_p = h2.find_next_sibling('p')
            if next_p:
                # Merge <p> tag content into <h2> and remove the <p> tag
                h2.string = f"{h2.get_text().strip()} {next_p.get_text().strip()}"
                next_p.decompose()

    # Replace formatted_content with the modified soup
    formatted_content = str(soup)

    chapter_content += formatted_content
    section_count += 1
    chapter_content += "\n</div>\n</body>"

    if section_count > 0:
        title_number_match = re.search(r'TITLE (\w+): (.+)', title_name)
        chapter_number_match = re.search(r'CHAPTER \d+(-?[A-Za-z]*)', safe_chapter_name)

        if title_number_match and chapter_number_match:
            title_number = title_number_match.group(1)
            chapter_number = chapter_number_match.group(0).split(" ")[1]  # Get the chapter number

            # Construct the filename using title_number and chapter_number
            xml_filename = os.path.join(Collection_path, f'{title_number}_{chapter_number}.xml')

            # Write chapter content to XML file
            with open(xml_filename, 'w', encoding='utf-8') as f:
                f.write(chapter_content)

            logger.info(f"Saved {section_count} sections in {xml_filename}")
            return xml_filename

        else:
            logger.error("Could not extract title or chapter number from the chapter name.")
            return None

    else:
        logger.warning(f"No sections found for {chapter_name}. Skipping.")
        return None




async def fetch_title(title_name, title_url):
    """Fetch and process each title."""
    logger.info(f"Downloading title: {title_name} from {title_url}")

    download_path = config['download_path']
    safe_title_name = title_name.replace(':', '-').replace('/', '-').strip()
    title_directory = os.path.join(download_path, safe_title_name)
    os.makedirs(title_directory, exist_ok=True)

    title_soup = await get_content(title_url)
    if not title_soup:
        return {}

    chapter_links = title_soup.find_all('a', href=True)

    # ✅ Fix: Keep exactly one "NHTOC/" in URL
    chapter_urls = {
        a.text.strip(): BASE_URL + a['href'].replace("../", "NHTOC/")  # Ensures correct format
        for a in chapter_links if 'NHTOC-' in a['href']
    }

    return {"title_name": title_name, "title_url": title_url, "chapters": chapter_urls, "directory": title_directory}



async def fetch_chapter(chapter_name, chapter_url, title_name, title_directory):
    """Fetch and process each chapter."""
    logger.info(f"Fetching chapter: {chapter_name} from {chapter_url}")

    chapter_soup = await get_content(chapter_url)
    if not chapter_soup:
        return None

    body_tag = chapter_soup.find('body')
    if not body_tag:
        logger.warning(f"Skipping {chapter_name} - No <body> tag found.")
        return None

    # ✅ Fix: Get the first valid link inside <body>
    section_link_tag = next(
        (a_tag for h2_tag in body_tag.find_all('h2') if (a_tag := h2_tag.find('a', href=True)) and "NHTOC-" in a_tag["href"]),
        None
    )

    if not section_link_tag:
        logger.warning(f"Skipping {chapter_name} - No valid section link found.")
        return None

    # ✅ Fix: Ensure only one "NHTOC/"
    section_url = BASE_URL + section_link_tag['href'].replace("../", "NHTOC/")
    logger.info(f"Found section link for {chapter_name}: {section_url}")

    return await fetch_section(section_url, chapter_name, title_name)


import os
import aiohttp
import asyncio
import logging
import re
import json
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from datetime import datetime, date
from Utilities.DB_connection import connect_to_mongodb
from Utilities.config import configureation

# Load configuration
config = configureation()

# Constants
BASE_URL = "https://gc.nh.gov/rsa/html/"
MAIN_URL = "https://gc.nh.gov/rsa/html/nhtoc.htm"
HEADERS = {'User-Agent': 'Mozilla/5.0'}

# Logger Setup
logger = logging.getLogger(__name__)

async def get_content(url):
    """Fetch content asynchronously from a URL"""
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(url, headers=HEADERS, timeout=180) as response:
                response.raise_for_status()
                return BeautifulSoup(await response.text(), 'html.parser')
        except Exception as e:
            logger.error(f"Unable to fetch content from {url}: {e}")
            return None

async def get_title_dict():
    """Fetch title dictionary from the main TOC page"""
    soup = await get_content(MAIN_URL)
    if not soup:
        return {}

    title_links = soup.find_all('a', href=True)
    return {
        a.text.strip(): BASE_URL + a['href'].replace("../", "NHTOC/")
        for a in title_links if 'NHTOC/' in a['href']
    }

async def fetch_title(title_name, title_url):
    """Fetch and process each title"""
    logger.info(f"Fetching title: {title_name} | URL: {title_url}")

    title_soup = await get_content(title_url)
    if not title_soup:
        logger.error(f"Failed to fetch HTML for {title_name}")
        return {}

    chapter_links = title_soup.find_all('a', href=True)
    chapter_urls = {
        a.text.strip(): BASE_URL + "NHTOC/" + a['href']
        for a in chapter_links if 'NHTOC-' in a['href']
    }

    return {"title_name": title_name, "title_url": title_url, "chapters": chapter_urls}

async def fetch_chapter(chapter_name, chapter_url, title_name, collection_path):
    """Fetch and process each chapter"""
    logger.info(f"Fetching chapter: {chapter_name} from {chapter_url}")

    chapter_soup = await get_content(chapter_url)
    if not chapter_soup:
        return None

    # Extract the section URL from the page
    section_link_tag = next(
        (a_tag for h2_tag in chapter_soup.find_all('h2') if (a_tag := h2_tag.find('a', href=True)) and "../" in a_tag["href"]),
        None
    )

    if not section_link_tag:
        logger.warning(f"Skipping {chapter_name}: No valid section link found.")
        return None

    section_url = BASE_URL + section_link_tag['href'].replace('../', '')
    return await fetch_section(section_url, chapter_name, title_name, collection_path)

async def fetch_section(section_url, chapter_name, title_name, collection_path):
    """Fetch section data and return formatted content"""
    section_soup = await get_content(section_url)
    if not section_soup:
        return None

    meta_tag = section_soup.find('meta', {'name': 'chapter'})
    if meta_tag:
        chapter_name = meta_tag['content'].strip()

    # Ensure correct title format (Roman numeral + chapter number)
    title_label = title_name.split()[1]  # Extract the Roman numeral
    chapter_number = re.search(r'CHAPTER (\d+[A-Z\-]*)', chapter_name, re.IGNORECASE)
    chapter_label = chapter_number.group(1) if chapter_number else "Unknown"

    filename = f"{title_label}_{chapter_label}.xml"
    file_path = os.path.join(collection_path, filename)

    # Extract content
    sections = section_soup.find_all('center')
    section_texts = []
    for section in sections:
        text = section.get_text("\n").strip()
        section_texts.append(text)

    # Save to XML format
    with open(file_path, "w", encoding="utf-8") as file:
        file.write("\n".join(section_texts))

    logger.info(f"Saved section {chapter_name} to {file_path}")

    return filename

async def download_NH_input(url, base_url, title, download, collection_path):
    """Download and process NH legal statutes and store in MongoDB"""
    collection_path = os.path.join(collection_path, f"Title_{title}")
    if not os.path.exists(collection_path):
        os.makedirs(collection_path)

    toc_list = await get_title_dict()

    if title:
        title_data = await fetch_title(title, toc_list.get(title, ""))
        if not title_data:
            return None

        with connect_to_mongodb() as client:
            logging.info(f"Connected to {config['docdb']} database")
            db = client[config['docdb']]
            collection = db[config['collection_name']]

            task_id = collection.count_documents({"taskId": {"$regex": "^NH"}}) + 1
            task_id = f"NH{task_id}"

            file_details = []
            for chapter_name, chapter_url in title_data["chapters"].items():
                chapter_file = await fetch_chapter(chapter_name, chapter_url, title_data["title_name"], collection_path)
                if chapter_file:
                    file_details.append({
                        "Name": chapter_file,
                        "collection": "true",
                        "common_conversion": "",
                        "mncr_conversion": "",
                        "meta": "",
                        "toc": "",
                        "type": "NH",
                        "titleName": title_data["title_name"],
                        "normCite": ""
                    })

            # Store in MongoDB
            title_details = {
                "jX": download['jX'],
                "createdBy": download['createdBy'],
                "createdDate": download['createdDate'].isoformat() if isinstance(download['createdDate'], date) else download['createdDate'],
                "titleNo": title,
                "titleName": download['Title'],
                "downloadDate": datetime.now().strftime("%Y-%m-%d"),
                "totalFiles": len(file_details),
                "fileDetail": file_details,
                "finalStage": "Conversion",
                "finalStatus": "In Progress",
                "taskId": task_id
            }

            collection.insert_one(title_details)
            logging.info(f"Title {title} inserted into {config['collection_name']} collection")

        return title_data

    return None

async def toc_NH():
    """Generate Table of Contents for NH"""
    toc_list = await get_title_dict()
    root = ET.Element("hierarchy")

    for key, val in toc_list.items():
        title_number = key.split(':')[0].strip().replace("TITLE", "").strip()
        title_name = key.split(':')[1].strip()

        child = ET.Element("chunk")
        child.set("label", title_number)
        child.set("title", title_name)
        root.append(child)

    return ET.tostring(root, encoding="unicode")





async def fetch_chapter(chapter_name, chapter_url, title_name, title_directory):
    """Fetch and process each chapter"""
    logger.info(f"Fetching chapter: {chapter_name} from {chapter_url}")

    chapter_soup = await get_content(chapter_url)
    if not chapter_soup:
        return None

    body_tag = chapter_soup.find('body')
    if not body_tag:
        logger.warning(f"Skipping {chapter_name}: No <body> tag found.")
        return None

    # 🔹 Extract the correct section link (e.g., l/1/1-mrg.htm)
    section_link_tag = next(
        (a_tag for a_tag in body_tag.find_all('a', href=True) if "/l/" in a_tag["href"]),
        None
    )

    if not section_link_tag:
        logger.warning(f"Skipping {chapter_name}: No valid section link found.")
        return None

    section_url = BASE_URL + section_link_tag['href'].replace('../', '')
    logger.info(f"Extracted section URL: {section_url}")

    return await fetch_section(section_url, chapter_name, title_name, title_directory)


async def fetch_section(section_url, chapter_name, title_name, title_directory):
    """Fetch section data and return formatted XML"""
    logger.info(f"Fetching section from {section_url}")

    section_soup = await get_content(section_url)
    if not section_soup:
        return None

    # Extract the meta tag content (if available)
    meta_tag = section_soup.find('meta', {'name': 'chapter'})
    if meta_tag:
        chapter_name = meta_tag['content'].strip()

    # Start building XML content
    chapter_content = CHAPTER_TEXT
    chapter_content += f'\n<p class="c1">{title_name}</p>'
    chapter_content += f'\n<p class="c1">{chapter_name}</p>'

    sections = section_soup.find_all('center')
    seen_sections = set()
    section_count = 0

    for section in sections:
        section_title_tag = section.find_next('b')
        if section_title_tag:
            section_text = section_title_tag.get_text(strip=True)

            if section_text in seen_sections:
                continue
            seen_sections.add(section_text)

            section_content = section.find_next('codesect')
            section_content_text = section_content.get_text("\n").strip() if section_content else "No content available"

            formatted_content = "\n".join([
                f"<h3>{line.strip()}</h3>" if re.match(r"^[a-zA-Z0-9]+\.", line.strip()) else f"<p>{line.strip()}</p>"
                for line in section_content_text.split("\n") if line.strip()
            ])

            chapter_content += f'\n<h1 class="c2">{section_text}</h1>'
            chapter_content += formatted_content

            source_note = section.find_next('sourcenote')
            if source_note:
                history_text = source_note.get_text(" ", strip=True)
                chapter_content += f'\n<p class="gc.nh.gov"><span class="history"> History: </span> {history_text}</p>'

            section_count += 1

    chapter_content += "\n</div>\n</body>"

    if section_count > 0:
        safe_chapter_name = chapter_name.replace(':', ' -').replace('/', '-').strip()
        xml_filename = os.path.join(title_directory, f"{safe_chapter_name}.xml")
        with open(xml_filename, 'w', encoding='utf-8') as f:
            f.write(chapter_content)
        logger.info(f"Saved {section_count} sections in {xml_filename}")
        return xml_filename
    else:
        logger.warning(f"No sections found for {chapter_name}. Skipping.")
        return None




import os
import json
import logging
import asyncio
from datetime import datetime, date
from pymongo import MongoClient
from Utilities.DB_connection import connect_to_mongodb

async def download_NH_input(url, base_url, title, download, collection_path):
    """
    Download and process NH legal statutes, extract content, and store it in MongoDB.
    """
    logger.info(f"Starting download for Title: {title}")

    # Define and create the collection path
    collection_path = os.path.join(collection_path, f"Title_{title}")
    if not os.path.exists(collection_path):
        os.makedirs(collection_path)

    # Fetch title URL from `fetch_title()`
    ack = await fetch_title(title, url)
    if not ack:
        logger.error(f"ERROR: Failed to fetch title {title}. URL might be incorrect.")
        return None

    title_url = ack.get("title_url", "")
    if not title_url:
        logger.error(f"ERROR: No valid URL found for title {title}")
        return None

    logger.info(f"Fetched title successfully: {title} | URL: {title_url}")

    with connect_to_mongodb() as client:
        logging.info(f"Connected to MongoDB: {config['docdb']} database")
        db = client[config['docdb']]
        collection = db[config['collection_name']]

        # Generate unique Task ID
        query = {"taskId": {"$regex": "^NH"}}
        task_id = collection.count_documents(query) + 1
        task_id = f'NH{task_id}'

        logger.info(f"Generated Task ID: {task_id}")

        # Get current date
        current_date = datetime.now().strftime("%Y-%m-%d")

        # Validate created date
        create_date = download.get('createdDate', current_date)
        if isinstance(create_date, date):
            create_date = create_date.isoformat()

        file_details = []

        for chapter_name, chapter_url in ack["chapters"].items():
            chapter_data = await fetch_chapter(chapter_name, chapter_url, ack["title_name"], collection_path)

            if chapter_data:
                # Extract chapter number format (e.g., 201, 201-A)
                chapter_match = re.search(r'CHAPTER (\d+[A-Z\-]*)', chapter_name, re.IGNORECASE)
                chapter_number = chapter_match.group(1) if chapter_match else "Unknown"

                # Keep Roman numerals in the title
                title_label = ack["title_name"].split(":")[0].replace("TITLE", "").strip()

                # Generate filename (e.g., III_201.xml, III_201_A.xml)
                filename = f"{title_label}_{chapter_number}.xml"

                details = {
                    "Name": filename,
                    "collection": "true",
                    "common_conversion": "",
                    "mncr_conversion": "",
                    "meta": "",
                    "toc": "",
                    "type": config['NH_seclevel'],
                    "titleName": chapter_name,
                    "normCite": ""
                }
                file_details.append(details)

                # Save XML content locally
                file_path = os.path.join(collection_path, filename)
                with open(file_path, "w", encoding="utf-8") as file:
                    json.dump(chapter_data, file, indent=4)

        # Insert task details in MongoDB
        title_details = {
            "jx": download["jX"],
            "createdBy": download["createdBy"],
            "createdDate": create_date,
            "titleNo": title,
            "titleName": download["Title"],
            "downloadDate": current_date,
            "totalFiles": len(file_details),
            "fileDetail": file_details,
            "finalStage": "Conversion",
            "finalStatus": "In Progress",
            "taskId": task_id
        }
        collection.insert_one(title_details)

        logging.info(f"Inserted Title {title} into MongoDB collection: {config['collection_name']}")

    client.close()
    logging.info(f"Title {title} Conversion in progress")

    return ack

new


async def download_NH_input(url, base_url, title, download, Collection_path):
    """Download and process NH legal statutes and store in MongoDB"""

    Collection_path = os.path.join(Collection_path, f"Title_{title}")
    if not os.path.exists(Collection_path):
        os.makedirs(Collection_path)

    # Fetch TOC List
    toc_list = await get_title_dict()

    if title:
        # Get title URL from fetch_title()
        title_url = toc_list.get(title, "")
        ack = await fetch_title(title, title_url)

        if not ack.get("chapters"):
            logger.error(f"ERROR: No chapters found for title {title}. URL might be incorrect")
            return None

        with connect_to_mongodb() as client:
            logging.info(f"Connected to {config['docdb']} database")
            db = client[config['docdb']]
            collection = db[config['collection_name']]

            # Create task ID
            query = {"taskId": {"$regex": "^NH"}}
            taskId = collection.count_documents(query) + 1
            taskId = f"NH{taskId}"
            logging.info(f"Generated Task ID: {taskId}")

            # Get current date
            formatted_date = datetime.now().strftime("%Y-%m-%d")
            createDate = download["createdDate"]
            if isinstance(createDate, date):
                createDate = createDate.isoformat()

            fileDetail = []

            for chapter_name, chapter_url in ack["chapters"].items():
                chapter_data = await fetch_chapter(chapter_name, chapter_url, ack["title_name"], ack["directory"])

                if chapter_data:
                    # Generate structured filename (TitleNumber_ChapterNumber.xml)
                    chapter_match = re.search(r'CHAPTER (\d+[A-Z\-]*)', chapter_name, re.IGNORECASE)
                    chapter_number = chapter_match.group(1) if chapter_match else "Unknown"
                    filename = f"{title}_{chapter_number}.xml"
                    file_path = os.path.join(Collection_path, filename)

                    # Save as XML file
                    with open(file_path, "w", encoding="utf-8") as file:
                        json.dump(chapter_data, file, indent=4)

                    details = {
                        "Name": filename,
                        "collection": "true",
                        "common_conversion": "",
                        "mncr_conversion": "",
                        "meta": "",
                        "toc": "",
                        "type": config["NH_seclevel"],
                        "titleName": chapter_name,
                        "normCite": ""
                    }
                    fileDetail.append(details)

            # Insert title details into MongoDB
            title_details = {
                "jX": download["jX"],
                "createdBy": download["createdBy"],
                "createdDate": createDate,
                "titleNo": title,
                "titleName": download["Title"],
                "downloadDate": formatted_date,
                "totalFiles": len(ack),
                "fileDetail": fileDetail,
                "finalStage": "Conversion",
                "finalStatus": "In Progress",
                "taskId": taskId
            }
            collection.insert_one(title_details)
            logging.info(f"Title {title} inserted into {config['collection_name']} collection")

        client.close()
        return ack

    else:
        for title_num, _ in toc_list.items():
            title_url = toc_list.get(title_num, "")
            message = await fetch_title(title_num, title_url)
        return message




async def download_NH_input(url, base_url, title, download, Collection_path):
    """Download and process NH legal statutes and store in MongoDB"""

    Collection_path = os.path.join(Collection_path, f"Title_{title}")
    if not os.path.exists(Collection_path):
        os.makedirs(Collection_path)

    # Fetch TOC List
    toc_list = await get_title_dict()

    if title:
        title_url = toc_list.get(title, "")
        ack = await fetch_title(title, title_url)

        if not ack.get("chapters"):
            logger.error(f"ERROR: No chapters found for title {title}. URL might be incorrect")
            return None

        with connect_to_mongodb() as client:
            logging.info(f"Connected to {config['docdb']} database")
            db = client[config['docdb']]
            collection = db[config['collection_name']]

            # Check if the title exists in MongoDB
            if (book := collection.find_one({"jx": "NH", "titleNo": title, "isDeleted": {"$exists": False}})) is not None:
                task_id = book["taskId"]
                query = {"taskId": task_id}
                file_details = []

                for chapter_name, chapter_url in ack["chapters"].items():
                    chapter_data = await fetch_chapter(chapter_name, chapter_url, ack["title_name"], ack["directory"])

                    if chapter_data:
                        # Extract Roman numeral title
                        title_number = title.strip()

                        # Extract chapter number
                        chapter_match = re.search(r'CHAPTER (\d+[A-Z\-]*)', chapter_name, re.IGNORECASE)
                        chapter_number = chapter_match.group(1) if chapter_match else "Unknown"

                        # Generate filename (e.g., III_201.xml, III_201_A.xml)
                        filename = f"{title_number}_{chapter_number}.xml"
                        file_path = os.path.join(Collection_path, filename)

                        # Save file in XML format
                        with open(file_path, "w", encoding="utf-8") as file:
                            json.dump(chapter_data, file, indent=4)

                        details = {
                            "Name": filename,
                            "collection": "true",
                            "common_conversion": "",
                            "mncr_conversion": "",
                            "meta": "",
                            "toc": "",
                            "type": config["NH_seclevel"],
                            "titleName": chapter_name,
                            "normCite": ""
                        }
                        file_details.append(details)

                # Update MongoDB records
                collection.update_one(query, {"$set": {"fileDetail": file_details}})
                collection.update_one(query, {"$set": {"totalFiles": len(file_details), "finalStage": "Conversion"}})

                logging.info(f"Title {title} inserted into {config['collection_name']} collection")

            client.close()
            logging.info(f"Title {title} Conversion in progress")

            # Send conversion request
            conv_url = config["convert_request"]
            payload = json.dumps({
                "data": {
                    "jx": "NH",
                    "createdBy": "",
                    "createdDate": datetime.now().strftime("%Y-%m-%d"),
                    "firstLeveno": f"{title}",
                    "firstLeveltitle": ""
                },
                "additionalParam": {
                    "convRawXm1": "yes",
                    "convMncrXm1": "string"
                }
            }, indent=4)

            headers = {"Content-Type": "application/json"}
            logging.info(f"Payload {payload}")

            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(conv_url, headers=headers, data=payload) as response:
                        request_resp = await response.text()
                        logging.info(f"Conversion request sent for {title}")
            except Exception as e:
                logging.error(f"Conversion request failed for {title} {e}")

        return ack

    else:
        for title_num in toc_list.keys():
            title_url = toc_list.get(title_num, "")
            message = await fetch_title(title_num, title_url)
        return message

import os
import aiohttp
import asyncio
import nest_asyncio
import json
import logging
import re
import xml.etree.ElementTree as ET
from datetime import datetime, date
from bs4 import BeautifulSoup
from pyppeteer import launch
from Utilities.logging_config import setup_logging
from Utilities.DB_connection import connect_to_mongodb
from Utilities.config import configureation

# Load configuration
config = configureation()
nest_asyncio.apply()

# Setup logging
setup_logging()
logger = logging.getLogger(__name__)

HEADERS = {'User-Agent': 'Mozilla/5.0'}

async def get_content(url):
    """Fetch content asynchronously using Pyppeteer (handles JavaScript rendering)"""
    try:
        browser = await launch(headless=True, args=['--no-sandbox'])
        page = await browser.newPage()
        await page.goto(url, {'waitUntil': 'networkidle2'})
        content = await page.content()
        await browser.close()
        return BeautifulSoup(content, 'html.parser')
    except Exception as e:
        logger.error(f"Failed to fetch content from {url}: {e}")
        return None

async def get_title_dict(url):
    """Fetch title dictionary from the main page dynamically"""
    soup = await get_content(url)
    if not soup:
        return {}

    title_links = soup.find_all('a', href=True)
    return {
        a.text.strip(): url + a['href'].replace("../", "NHTOC/")
        for a in title_links if 'NHTOC/' in a['href']
    }

async def fetch_title(title_name, title_url, base_url):
    """Fetch and process each title"""
    logger.info(f"Downloading title: {title_name}")
    title_soup = await get_content(title_url)
    if not title_soup:
        return {}

    chapter_links = title_soup.find_all('a', href=True)
    chapter_urls = {
        a.text.strip(): base_url + "NHTOC/" + a['href']
        for a in chapter_links if 'NHTOC-' in a['href']
    }

    return {"title_name": title_name, "title_url": title_url, "chapters": chapter_urls}

async def fetch_chapter(chapter_name, chapter_url, title_name):
    """Fetch and process each chapter"""
    logger.info(f"Fetching chapter: {chapter_name}")
    chapter_soup = await get_content(chapter_url)
    if not chapter_soup:
        return None

    body_tag = chapter_soup.find('body')
    if not body_tag:
        logger.warning(f"Skipping {chapter_name} - No <body> tag found.")
        return None

    section_link_tag = next(
        (a_tag for h2_tag in body_tag.find_all('h2') if (a_tag := h2_tag.find('a', href=True)) and "../" in a_tag["href"]), 
        None
    )

    if not section_link_tag:
        logger.warning(f"Skipping {chapter_name} - No valid section link found.")
        return None

    section_url = section_link_tag['href'].replace('../', '')
    return await fetch_section(section_url, chapter_name, title_name)

async def fetch_section(section_url, chapter_name, title_name):
    """Fetch section data and return formatted content for MongoDB storage"""
    section_soup = await get_content(section_url)
    if not section_soup:
        return None

    meta_tag = section_soup.find('meta', {'name': 'chapter'})
    if meta_tag:
        chapter_name = meta_tag['content'].strip()

    chapter_content = {
        "title_name": title_name,
        "chapter_name": chapter_name,
        "sections": []
    }

    sections = section_soup.find_all('center')
    seen_sections = set()

    for section in sections:
        section_title_tag = section.find_next('b')
        if section_title_tag:
            section_text = section_title_tag.get_text(strip=True)

            if section_text in seen_sections:
                continue
            seen_sections.add(section_text)

            section_content = section.find_next('codesect')
            section_content_text = section_content.get_text("\n").strip() if section_content else "No content available"

            formatted_content = [
                {"type": "h3", "text": line.strip()} if re.match(r"^?[a-z0-9]+?\.", line.strip()) else {"type": "p", "text": line.strip()}
                for line in section_content_text.split("\n") if line.strip()
            ]

            chapter_content["sections"].append({
                "section_title": section_text,
                "content": formatted_content
            })

    return chapter_content if chapter_content["sections"] else None

async def download_NH_input(url, base_url, title, download, collection_path):
    """Download and process NH legal statutes and store in MongoDB"""

    collection_path = os.path.join(collection_path, f"Title_{title}")
    if not os.path.exists(collection_path):
        os.makedirs(collection_path)

    toc_list = await get_title_dict(url)

    if title:
        title_url = f"{base_url}NHTOC/nhtoc_ch{title}.htm"
        ack = await fetch_title(title, title_url, base_url)

        with connect_to_mongodb() as client:
            logging.info(f"Connected to {config['docdb']} database")
            db = client[config['docdb']]
            collection = db[config['collection_name']]

            if (book := collection.find_one({"jx": 'NH', "titleNo": title, "isDeleted": {"$exists": False}})) is not None:
                task_id = book["taskId"]
                query = {"taskId": task_id}
                file_details = []

                for chapter_name, chapter_url in ack["chapters"].items():
                    chapter_data = await fetch_chapter(chapter_name, chapter_url, ack["title_name"])
                    
                    if chapter_data:
                        filename = f"title_{chapter_name}.xml"

                        details = {
                            "Name": filename,
                            "collection": "true",
                            "common_conversion": "",
                            "mncr_conversion": "",
                            "meta": "",
                            "toc": "",
                            "type": config['NH_seclevel'],
                            "titleName": chapter_name,
                            "normCite": ""
                        }
                        file_details.append(details)

                        file_path = os.path.join(collection_path, filename)
                        with open(file_path, "w", encoding="utf-8") as file:
                            json.dump(chapter_data, file, indent=4)

                collection.update_one(query, {"$set": {"fileDetail": file_details}})
                collection.update_one(query, {"$set": {"totalFiles": len(file_details), "finalStage": "Conversion"}})

                logging.info(f"Title {title} inserted into {config['collection_name']} collection")

            client.close()
            logging.info(f"Title {title} Conversion in progress")

async def toc_NH(url):
    toc_list = await get_title_dict(url)
    root = ET.Element("hierarchy")

    for key, val in toc_list.items():
        title_number = key.split(':')[0].strip().replace("TITLE", "").strip()
        title_name = key.split(':')[1].strip()

        child = ET.Element("chunk")
        child.set("label", title_number)
        child.set("title", title_name)
        root.append(child)

    return ET.tostring(root, encoding="unicode")
