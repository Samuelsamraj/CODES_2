


import aiohttp
import os
import json
import logging
import asyncio
import nest_asyncio
import requests
import re
import xml.etree.ElementTree as ET
from datetime import datetime, date
from bs4 import BeautifulSoup
from fastapi import FastAPI
from pymongo import MongoClient
from Utilities.config import configureation
from Utilities.logging_config import setup_logging
from Utilities.DB_connection import connect_to_mongodb

# Setup logging
setup_logging()
logger = logging.getLogger(__name__)

# Load configuration
config_yml = configureation()
config_data = requests.get(config_yml['config_path'])
config_da = config_data.json()
config = config_da['propertySources'][0]['source']

# Apply nested event loop
nest_asyncio.apply()

# Constants
BASE_URL = "https://gc.nh.gov/rsa/html/"
MAIN_URL = "https://gc.nh.gov/rsa/html/nhtoc.htm"
HEADERS = {'User-Agent': 'Mozilla/5.0'}

# Standard XML Header for Chapters
CHAPTER_TEXT = """<?xml version="1.0"?>
<body><div class="WordSection1">"""

# Function to fetch content from a given URL using aiohttp
async def get_content(url):
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=HEADERS, timeout=180) as response:
                response.raise_for_status()
                return BeautifulSoup(await response.text(), 'html.parser')
    except Exception as e:
        logger.error(f"Unable to fetch content from {url}: {e}")
        return None

# Fetch and parse the Table of Contents (TOC)
async def get_title_dict():
    soup = await get_content(MAIN_URL)
    if not soup:
        return {}

    title_links = soup.find_all('a', href=True)
    return {
        a.text.strip(): BASE_URL + a['href'].replace("../", "NHTOC/")
        for a in title_links if 'NHTOC/' in a['href']
    }

# Fetch and process each title
async def fetch_title(title_name, title_url):
    logger.info(f"Downloading title: {title_name}")

    title_soup = await get_content(title_url)
    if not title_soup:
        return {}

    chapter_links = title_soup.find_all('a', href=True)
    chapter_urls = {
        a.text.strip(): BASE_URL + "NHTOC/" + a['href']
        for a in chapter_links if 'NHTOC-' in a['href']
    }

    return {"title_name": title_name, "title_url": title_url, "chapters": chapter_urls}

# Fetch and process each chapter
async def fetch_chapter(chapter_name, chapter_url, title_name):
    logger.info(f"Fetching chapter: {chapter_name}")

    chapter_soup = await get_content(chapter_url)
    if not chapter_soup:
        return None

    body_tag = chapter_soup.find('body')
    if not body_tag:
        logger.warning(f"Skipping {chapter_name} - No <body> tag found.")
        return None

    section_link_tag = next(
        (a_tag for h2_tag in body_tag.find_all('h2') if (a_tag := h2_tag.find('a', href=True)) and "../" in a_tag["href"]), 
        None
    )

    if not section_link_tag:
        logger.warning(f"Skipping {chapter_name} - No valid section link found.")
        return None

    section_url = BASE_URL + section_link_tag['href'].replace('../', '')
    return await fetch_section(section_url, chapter_name, title_name)

# Fetch section data for storage
async def fetch_section(section_url, chapter_name, title_name):
    section_soup = await get_content(section_url)
    if not section_soup:
        return None

    meta_tag = section_soup.find('meta', {'name': 'chapter'})
    if meta_tag:
        chapter_name = meta_tag['content'].strip()

    chapter_content = {
        "title_name": title_name,
        "chapter_name": chapter_name,
        "sections": []
    }

    sections = section_soup.find_all('center')
    seen_sections = set()

    for section in sections:
        section_title_tag = section.find_next('b')
        if section_title_tag:
            section_text = section_title_tag.get_text(strip=True)

            if section_text in seen_sections:
                continue
            seen_sections.add(section_text)

            section_content = section.find_next('codesect')
            section_content_text = section_content.get_text("\n").strip() if section_content else "No content available"

            formatted_content = [
                {"type": "h3", "text": line.strip()} if re.match(r"^?[a-z0-9]+?\.", line.strip()) else {"type": "p", "text": line.strip()}
                for line in section_content_text.split("\n") if line.strip()
            ]

            chapter_content["sections"].append({
                "section_title": section_text,
                "content": formatted_content
            })

    return chapter_content if chapter_content["sections"] else None

# Download and process NH legal statutes
async def download_NH_input(url, base_url, title, download, Collection_path):
    Collection_path = os.path.join(Collection_path, f"Title_{title}")
    if not os.path.exists(Collection_path):
        os.makedirs(Collection_path)

    toc_list = await get_title_dict()

    if title:
        title_url = f"{BASE_URL}NHTOC/nhtoc_ch{title}.htm"
        ack = await fetch_title(title, title_url)

        with connect_to_mongodb() as client:
            logging.info(f"Connected to {config['docdb']} database")
            db = client[config['docdb']]
            collection = db[config['collection_name']]

            if (book := collection.find_one({"jx": 'NH', "titleNo": title, "isDeleted": {"$exists": False}})) is not None:
                task_id = book["taskId"]
                query = {"taskId": task_id}
                file_details = []

                for chapter_name, chapter_url in ack["chapters"].items():
                    chapter_data = await fetch_chapter(chapter_name, chapter_url, ack["title_name"])
                    
                    if chapter_data:
                        filename = f"{title}_{chapter_name}.xml"
                        details = {
                            "Name": filename,
                            "collection": "true",
                            "common_conversion": "",
                            "mncr_conversion": "",
                            "meta": "",
                            "toc": "",
                            "type": config['NH_seclevel'],
                            "titleName": chapter_name,
                            "normCite": ""
                        }
                        file_details.append(details)

                        file_path = os.path.join(Collection_path, filename)
                        with open(file_path, "w", encoding="utf-8") as file:
                            json.dump(chapter_data, file, indent=4)

                collection.update_one(query, {"$set": {"fileDetail": file_details}})
                collection.update_one(query, {"$set": {"totalFiles": len(file_details), "finalStage": "Conversion"}})

                logging.info(f"Title {title} inserted into {config['collection_name']} collection")

            client.close()
            logging.info(f"Title {title} Conversion in progress")

    else:
        for title_num in toc_list.keys():
            title_url = f"{BASE_URL}NHTOC/nhtoc_ch{title_num}.htm"
            message = await fetch_title(title_num, title_url)
        return message

# Generate TOC for NH
async def toc_NH():
    toc_list = await get_title_dict()
    root = ET.Element("hierarchy")

    for key, val in toc_list.items():
        title_number = key.split(':')[0].strip()
        title_name = key.split(':')[1].strip()

        child = ET.Element("chunk")
        child.set("label", title_number)
        child.set("title", title_name)
        root.append(child)

    return ET.tostring(root, encoding="unicode")





2nd code : 


import aiohttp
import os
import asyncio
import nest_asyncio
import json
import logging
import re
import xml.etree.ElementTree as ET
from datetime import datetime, date
from bs4 import BeautifulSoup
from pyppeteer import launch
from pymongo import MongoClient
from Utilities.logging_config import setup_logging
from Utilities.DB_connection import connect_to_mongodb
from Utilities.config import configureation
from fastapi import FastAPI
import requests

# Setup logging
setup_logging()
logger = logging.getLogger(__name__)

# Load configuration
config = configureation()
nest_asyncio.apply()

# Fetch configuration values dynamically
config_data = requests.get(config['config_path'])
config_da = config_data.json()
config = config_da['propertySources'][0]['source']

HEADERS = {'User-Agent': 'Mozilla/5.0'}
CHAPTER_TEXT = """<?xml version="1.0"?><body><div class="WordSection1">"""

# Function to fetch and render HTML content using pyppeteer
async def get_content(url, Collection_path):
    """Fetch content asynchronously using pyppeteer for better accuracy"""
    try:
        browser = await launch(headless=True, args=['--no-sandbox'])
        page = await browser.newPage()
        await page.setUserAgent(HEADERS['User-Agent'])
        await page.goto(url, {'waitUntil': 'networkidle2'})
        content = await page.content()
        await browser.close()
        return BeautifulSoup(content, 'html.parser')
    except Exception as e:
        logger.error(f"Unable to fetch content from {url}: {e}")
        return None

# Fetch title dictionary from main page
async def get_title_dict(url, Collection_path):
    """Fetches a list of titles from the NH main page"""
    soup = await get_content(url, Collection_path)
    if not soup:
        return {}

    title_links = soup.find_all('a', href=True)
    return {
        a.text.strip(): url + a['href'].replace("../", "NHTOC/")
        for a in title_links if 'NHTOC/' in a['href']
    }

# Extract chapter details from a title page
async def fetch_title(title_url, title, toc_list, Collection_path):
    """Extracts chapter links for a given title"""
    logger.info(f"Fetching title: {title}")

    title_soup = await get_content(title_url, Collection_path)
    if not title_soup:
        return {}

    chapter_links = title_soup.find_all('a', href=True)
    chapter_urls = {
        a.text.strip(): title_url + a['href']
        for a in chapter_links if 'NHTOC-' in a['href']
    }

    return {"title_name": title, "title_url": title_url, "chapters": chapter_urls}

# Fetch and process a chapter
async def fetch_chapter(chapter_url, chapter_name, title, Collection_path):
    """Extracts section details from a chapter"""
    logger.info(f"Fetching chapter: {chapter_name}")
    chapter_soup = await get_content(chapter_url, Collection_path)

    if not chapter_soup:
        return None

    section_link_tag = next(
        (a_tag for h2_tag in chapter_soup.find_all('h2') if (a_tag := h2_tag.find('a', href=True)) and "../" in a_tag["href"]),
        None
    )

    if not section_link_tag:
        logger.warning(f"Skipping {chapter_name} - No valid section link found.")
        return None

    section_url = chapter_url + section_link_tag['href'].replace('../', '')
    return await fetch_section(section_url, chapter_name, title, Collection_path)

# Fetch and process a section
async def fetch_section(section_url, chapter_name, title, Collection_path):
    """Extracts content for sections inside a chapter"""
    section_soup = await get_content(section_url, Collection_path)
    if not section_soup:
        return None

    sections = section_soup.find_all('center')
    chapter_content = {"title_name": title, "chapter_name": chapter_name, "sections": []}

    for section in sections:
        section_title_tag = section.find_next('b')
        if section_title_tag:
            section_text = section_title_tag.get_text(strip=True)

            section_content = section.find_next('codesect')
            section_content_text = section_content.get_text("\n").strip() if section_content else "No content available"

            formatted_content = [
                {"type": "h3", "text": line.strip()} if re.match(r"^[a-z0-9]+?\.", line.strip()) else {"type": "p", "text": line.strip()}
                for line in section_content_text.split("\n") if line.strip()
            ]

            chapter_content["sections"].append({"section_title": section_text, "content": formatted_content})

    return chapter_content if chapter_content["sections"] else None

# Download NH input and save content dynamically
async def download_NH_input(url, base_url, title, download, Collection_path):
    """Extracts NH legal data dynamically and stores it in MongoDB"""
    toc_list = await get_title_dict(url, Collection_path)
    title_url = f"{base_url}NHTOC/nhtoc_ch{title}.htm"
    ack = await fetch_title(title_url, title, toc_list, Collection_path)

    with connect_to_mongodb() as client:
        db = client[config['docdb']]
        collection = db[config['collection_name']]

        if (book := collection.find_one({"jx": 'NH', "titleNo": title, "isDeleted": {"$exists": False}})) is not None:
            task_id = book["taskId"]
            query = {"taskId": task_id}
            file_details = []

            for chapter_name, chapter_url in ack["chapters"].items():
                chapter_data = await fetch_chapter(chapter_url, chapter_name, ack["title_name"], Collection_path)
                if chapter_data:
                    filename = f"{title}_{chapter_name}.xml"
                    details = {"Name": filename, "type": config['NH_seclevel'], "titleName": chapter_name}
                    file_details.append(details)

            collection.update_one(query, {"$set": {"fileDetail": file_details}})
            collection.update_one(query, {"$set": {"totalFiles": len(file_details), "finalStage": "Conversion"}})

    return ack

# Validate NH input dynamically
async def validate_NH_input(url, base_url, download, Collection_path):
    """Validates NH input data before processing"""
    with connect_to_mongodb() as client:
        db = client[config['docdb']]
        collection = db[config['collection_name']]
        done, wip = [], []

        for tn in download.download:
            title = tn.firstLeveno
            if (book := collection.find_one({"jx": 'NH', "titleNo": title, "isDeleted": {"$exists": False}})) is not None:
                done.append({"title": title, "downloadDate": book["downloadDate"]})
            else:
                wip.append({"Num": title, "jx": download.jX})

        for wip_data in wip:
            title = wip_data['Num']
            asyncio.create_task(download_NH_input(url, base_url, title, download, Collection_path))

    return done, wip

# Generate TOC dynamically
async def toc_NH(url, Collection_path):
    """Generates structured XML TOC for NH dynamically"""
    toc_list = await get_title_dict(url, Collection_path)
    root = ET.Element("hierarchy")

    for key, val in toc_list.items():
        child = ET.Element("chunk")
        child.set("label", key.split(':')[0].strip())
        child.set("title", key.split(':')[1].strip())
        root.append(child)

    return ET.tostring(root, encoding="unicode")




3rd code 

import sys
import os
import aiohttp
import asyncio
import nest_asyncio
import json
import logging
import re
import xml.etree.ElementTree as ET
from datetime import datetime, date
from bs4 import BeautifulSoup
from pyppeteer import launch, errors
from lxml import etree
import requests
from fastapi import FastAPI
from Utilities.logging_config import setup_logging
from Utilities.DB_connection import connect_to_mongodb
from Utilities.config import configureation

# Load configuration
config = configureation()
nest_asyncio.apply()
setup_logging()
logger = logging.getLogger(__name__)

# Constants
HEADERS = {'User-Agent': 'Mozilla/5.0'}

async def get_content(url):
    """Fetch content asynchronously from a URL"""
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(url, headers=HEADERS, timeout=180) as response:
                response.raise_for_status()
                return BeautifulSoup(await response.text(), 'html.parser')
        except Exception as e:
            logger.error(f"Unable to fetch content from {url}: {e}")
            return None

async def get_title_dict(main_url):
    """Fetch title dictionary from the main page"""
    soup = await get_content(main_url)
    if not soup:
        return {}
    
    title_links = soup.find_all('a', href=True)
    return {
        a.text.strip(): main_url + a['href'].replace("../", "NHTOC/")
        for a in title_links if 'NHTOC/' in a['href']
    }

async def fetch_title(title_name, title_url):
    """Fetch and process each title"""
    logger.info(f"Downloading title: {title_name}")
    
    title_soup = await get_content(title_url)
    if not title_soup:
        return {}
    
    chapter_links = title_soup.find_all('a', href=True)
    chapter_urls = {
        a.text.strip(): title_url + "NHTOC/" + a['href']
        for a in chapter_links if 'NHTOC-' in a['href']
    }
    return {"title_name": title_name, "title_url": title_url, "chapters": chapter_urls}

async def fetch_chapter(chapter_name, chapter_url):
    """Fetch and process each chapter"""
    logger.info(f"Fetching chapter: {chapter_name}")
    
    chapter_soup = await get_content(chapter_url)
    if not chapter_soup:
        return None
    
    body_tag = chapter_soup.find('body')
    if not body_tag:
        logger.warning(f"Skipping {chapter_name} - No <body> tag found.")
        return None
    
    section_link_tag = next(
        (a_tag for h2_tag in body_tag.find_all('h2') if (a_tag := h2_tag.find('a', href=True)) and "../" in a_tag["href"]),
        None
    )
    
    if not section_link_tag:
        logger.warning(f"Skipping {chapter_name} - No valid section link found.")
        return None
    
    section_url = chapter_url + section_link_tag['href'].replace('../', '')
    return await fetch_section(section_url, chapter_name)

async def fetch_section(section_url, chapter_name):
    """Fetch section data and return formatted content"""
    section_soup = await get_content(section_url)
    if not section_soup:
        return None
    
    chapter_content = {
        "chapter_name": chapter_name,
        "sections": []
    }
    
    sections = section_soup.find_all('center')
    for section in sections:
        section_title_tag = section.find_next('b')
        if section_title_tag:
            section_text = section_title_tag.get_text(strip=True)
            section_content = section.find_next('codesect')
            section_content_text = section_content.get_text("\n").strip() if section_content else "No content available"
            formatted_content = [
                {"type": "h3", "text": line.strip()} if re.match(r"^?[a-z0-9]+?\.", line.strip()) else {"type": "p", "text": line.strip()}
                for line in section_content_text.split("\n") if line.strip()
            ]
            chapter_content["sections"].append({
                "section_title": section_text,
                "content": formatted_content
            })
    return chapter_content

async def download_NH_input(url, title, collection_path):
    """Download and process NH legal statutes and store in MongoDB"""
    collection_path = os.path.join(collection_path, f"Title_{title}")
    os.makedirs(collection_path, exist_ok=True)
    
    title_url = url + f"NHTOC/nhtoc_ch{title}.htm"
    ack = await fetch_title(title, title_url)
    
    with connect_to_mongodb() as client:
        logging.info(f"Connected to {config['docdb']} database")
        db = client[config['docdb']]
        collection = db[config['collection_name']]
        
        file_details = []
        for chapter_name, chapter_url in ack["chapters"].items():
            chapter_data = await fetch_chapter(chapter_name, chapter_url)
            if chapter_data:
                filename = f"{title}_{chapter_name}.xml"
                details = {
                    "Name": filename,
                    "collection": "true",
                    "type": config['NH_seclevel'],
                    "titleName": chapter_name
                }
                file_details.append(details)
                with open(os.path.join(collection_path, filename), "w", encoding="utf-8") as file:
                    json.dump(chapter_data, file, indent=4)
        
        collection.update_one({"titleNo": title}, {"$set": {"fileDetail": file_details}})
        client.close()
    return ack

async def toc_NH(main_url):
    """Generate TOC for NH"""
    toc_list = await get_title_dict(main_url)
    root = ET.Element("hierarchy")
    for key, val in toc_list.items():
        child = ET.Element("chunk")
        child.set("label", key.split(':')[0].strip().replace("TITLE", "").strip())
        child.set("title", key.split(':')[1].strip())
        root.append(child)
    return ET.tostring(root, encoding="unicode")

