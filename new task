



import os
import re
import ssl
import logging
import aiohttp
import asyncio
from bs4 import BeautifulSoup
from lxml import etree

# ---------------------------- #
# ðŸ”¹ Logger Setup
# ---------------------------- #
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ---------------------------- #
# ðŸ”¹ Constants & Headers
# ---------------------------- #
BASE_URL = "https://webserver.rilegislature.gov/Statutes/"
HEADERS = {'User-Agent': 'Mozilla/5.0'}

# SSL Context (Avoids SSL Verification Issues)
ssl_context = ssl.create_default_context()
ssl_context.check_hostname = False
ssl_context.verify_mode = ssl.CERT_NONE

# XML Header Template
CHAPTER_TEXT = """<?xml version="1.0"?>
<body>
<div class="WordSection1">
"""

# ---------------------------- #
# ðŸ”¹ Function 1: Fetch Webpage Content Asynchronously
# ---------------------------- #
async def get_content(url):
    """Fetch content asynchronously from a URL."""
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(url, headers=HEADERS, timeout=180, ssl=ssl_context) as response:
                response.raise_for_status()
                return BeautifulSoup(await response.text(), 'html.parser')
        except Exception as e:
            logger.error(f"Unable to fetch content from {url}: {e}")
            return None

# ---------------------------- #
# ðŸ”¹ Function 2: Extract All Titles
# ---------------------------- #
async def get_title_dict(url):
    """Fetch all titles from the main page."""
    soup = await get_content(url)

    if not soup:
        return {}

    title_links = {
        a.text.strip(): BASE_URL + a['href']
        for a in soup.find_all('a', href=True) if 'TITLE' in a['href']
    }

    return title_links

# ---------------------------- #
# ðŸ”¹ Function 3: Extract Chapters for a Given Title
# ---------------------------- #
async def fetch_title(title_url, title, Collection_path):
    """Fetch and process each title."""
    
    logger.info(f"Downloading title: {title}")
    title_soup = await get_content(title_url)

    if not title_soup:
        return {}

    chapter_urls = {
        a.text.strip(): title_url.rsplit("/", 1)[0] + "/" + a['href']
        for a in title_soup.find_all('a', href=True) if '.htm' in a['href']
    }

    return {
        "title_name": title,
        "title_url": title_url,
        "chapters": chapter_urls,
        "directory": Collection_path
    }

# ---------------------------- #
# ðŸ”¹ Function 4: Extract Sections for a Given Chapter
# ---------------------------- #
async def fetch_chapter(chapter_name, chapter_url, title_name, Collection_path):
    """Fetch and process each chapter."""
    
    logger.info(f"Fetching chapter: {chapter_name}")
    chapter_soup = await get_content(chapter_url)

    if not chapter_soup:
        return None

    body_tag = chapter_soup.find('body')
    if not body_tag:
        logger.warning(f"Skipping {chapter_name}: No <body> tag found.")
        return None

    section_links = {
        a.text.strip(): chapter_url.rsplit("/", 1)[0] + "/" + a['href']
        for a in body_tag.find_all('a', href=True) if '.htm' in a['href']
    }

    return await fetch_section(section_links, chapter_name, title_name, Collection_path)

# ---------------------------- #
# ðŸ”¹ Function 5: Fetch & Format Section Data
# ---------------------------- #
async def fetch_section(section_links, chapter_name, title_name, Collection_path):
    """Fetch section data and return formatted XML."""
    
    section_data = {}

    for section_name, section_url in section_links.items():
        logger.info(f"Fetching section: {section_name} - {section_url}")

        section_soup = await get_content(section_url)

        if not section_soup:
            logger.warning(f"Skipping {section_name}: Unable to fetch content.")
            continue

        # Extract title & chapter names from meta tags
        meta_tag_title = section_soup.find('meta', {'name': 'title'})
        meta_tag_chapter = section_soup.find('meta', {'name': 'chapter'})

        safe_title_name = meta_tag_title['content'].strip().replace(":", ".") if meta_tag_title else title_name
        safe_chapter_name = meta_tag_chapter['content'].strip().replace(":", "-").replace("/", "-") if meta_tag_chapter else chapter_name

        chapter_content = CHAPTER_TEXT
        dl_counter = 1

        # Add title and chapter names
        chapter_content += f'\n<p class="c1" dl="{dl_counter}">{safe_title_name}</p>'
        dl_counter += 1
        chapter_content += f'\n<p class="c1" dl="{dl_counter}">{safe_chapter_name}</p>'
        dl_counter += 1

        formatted_content = ""

        sections = section_soup.find_all('center')
        seen_sections = set()

        for section in sections:
            section_title_tag = section.find_next('b')
            if section_title_tag:
                section_text = section_title_tag.get_text(strip=True).replace("-", "")
                if section_text in seen_sections:
                    continue
                seen_sections.add(section_text)

                section_content = section.find_next('codesect')
                section_content_text = section_content.get_text("\n").strip() if section_content else "No content available"

                chapter_content += f'\n<h1 class="c2">{section_text}</h1>'

                lines = section_content_text.split("\n")
                for line in lines:
                    line = line.strip()
                    if not line:
                        continue

                    # Match sections, subsections, and numbered items
                    if re.match(r'^(Article [IVXLCDM]+)', line):
                        formatted_content += f'\n<h2>{line}</h2>'
                    elif re.match(r'^[A-Z]+\.$', line):
                        formatted_content += f'\n<h2>{line}</h2>'
                    elif re.match(r'^\([a-zA-Z]+\)', line):
                        formatted_content += f'\n<h3>{line}</h3>'
                    elif re.match(r'^\d+\.', line):
                        formatted_content += f'\n<h4>{line}</h4>'
                    else:
                        formatted_content += f'\n<p>{line}</p>'

        chapter_content += formatted_content
        chapter_content += "\n</div>\n</body>"

        # Save XML file
        os.makedirs(Collection_path, exist_ok=True)
        xml_filename = os.path.join(Collection_path, f"{title_name}_{chapter_name}_{section_name}.xml")

        with open(xml_filename, 'w', encoding='utf-8') as f:
            f.write(chapter_content)

        logger.info(f"Saved section in {xml_filename}")
        section_data[section_name] = xml_filename

    return section_data


async def fetch_section(section_links, chapter_name, title_name, Collection_path):
    """Fetch section data and return formatted XML."""

    section_data = {}

    for section_name, section_url in section_links.items():
        logger.info(f"Fetching section: {section_name} - {section_url}")

        section_soup = await get_content(section_url)

        if not section_soup:
            logger.warning(f"Skipping {section_name}: Unable to fetch content.")
            continue

        # Extract title & chapter names from meta tags
        meta_tag_title = section_soup.find('meta', {'name': 'title'})
        meta_tag_chapter = section_soup.find('meta', {'name': 'chapter'})

        safe_title_name = meta_tag_title['content'].strip().replace(":", ".") if meta_tag_title else title_name
        safe_chapter_name = meta_tag_chapter['content'].strip().replace(":", "-").replace("/", "-") if meta_tag_chapter else chapter_name

        chapter_content = CHAPTER_TEXT
        dl_counter = 1

        # Add title and chapter names
        chapter_content += f'\n<p class="c1" dl="{dl_counter}">{safe_title_name}</p>'
        dl_counter += 1
        chapter_content += f'\n<p class="c1" dl="{dl_counter}">{safe_chapter_name}</p>'
        dl_counter += 1

        formatted_content = ""

        # Extract content inside the <body> tag
        body_tag = section_soup.find('body')
        if not body_tag:
            logger.warning(f"Skipping {section_name}: No <body> tag found.")
            continue

        # Extract all text inside <div>, <p>, <center>, <h1>, and <h3>
        all_content = body_tag.find_all(['div', 'p', 'center', 'h1', 'h3'])

        for content in all_content:
            tag_name = content.name  # Get the tag name (div, p, center, h1, h3)
            text = content.get_text("\n", strip=True)

            # Convert bold text to <b> format
            bold_text = content.find('b')
            if bold_text:
                text = f"<b>{bold_text.get_text(strip=True)}</b>"

            # Preserve original structure
            if tag_name == "h1":
                formatted_content += f'\n<h1>{text}</h1>'
            elif tag_name == "h3":
                formatted_content += f'\n<h3>{text}</h3>'
            elif tag_name == "center":
                formatted_content += f'\n<center>{text}</center>'
            elif tag_name == "p":
                formatted_content += f'\n<p>{text}</p>'
            elif tag_name == "div":
                formatted_content += f'\n<div>{text}</div>'

        chapter_content += formatted_content
        chapter_content += "\n</div>\n</body>"

        # Save XML file
        os.makedirs(Collection_path, exist_ok=True)
        xml_filename = os.path.join(Collection_path, f"{title_name}_{chapter_name}_{section_name}.xml")

        with open(xml_filename, 'w', encoding='utf-8') as f:
            f.write(chapter_content)

        logger.info(f"Saved section in {xml_filename}")
        section_data[section_name] = xml_filename

    return section_data







async def fetch_section(section_links, chapter_name, title_name, Collection_path):
    """Fetch section data and return formatted XML."""

    section_data = {}

    for section_name, section_url in section_links.items():
        logger.info(f"Fetching section: {section_name} - {section_url}")

        section_soup = await get_content(section_url)

        if not section_soup:
            logger.warning(f"Skipping {section_name}: Unable to fetch content.")
            continue

        # Extract title & chapter names from meta tags
        meta_tag_title = section_soup.find('meta', {'name': 'title'})
        meta_tag_chapter = section_soup.find('meta', {'name': 'chapter'})

        safe_title_name = meta_tag_title['content'].strip().replace(":", ".") if meta_tag_title else title_name
        safe_chapter_name = meta_tag_chapter['content'].strip().replace(":", "-").replace("/", "-") if meta_tag_chapter else chapter_name

        chapter_content = CHAPTER_TEXT
        dl_counter = 1

        # Add title and chapter names
        chapter_content += f'\n<p class="c1" dl="{dl_counter}">{safe_title_name}</p>'
        dl_counter += 1
        chapter_content += f'\n<p class="c1" dl="{dl_counter}">{safe_chapter_name}</p>'
        dl_counter += 1

        formatted_content = ""

        # Extract content inside the <body> tag
        body_tag = section_soup.find('body')
        if not body_tag:
            logger.warning(f"Skipping {section_name}: No <body> tag found.")
            continue

        # Extract all content inside <div>, <p>, <center>, <h1>, and <h3>
        all_content = body_tag.find_all(['div', 'p', 'center', 'h1', 'h3'])

        for content in all_content:
            tag_name = content.name  # Get the tag name (div, p, center, h1, h3)

            # Extract text including bold (<b>) and normal text
            bold_text = content.find('b')
            normal_text = content.get_text(" ", strip=True)  # Extracts everything inside the tag

            # Ensure bold text remains inside <b> while keeping full paragraph
            if bold_text:
                formatted_text = f"<b>{bold_text.get_text(strip=True)}</b> {normal_text.replace(bold_text.get_text(strip=True), '').strip()}"
            else:
                formatted_text = normal_text  # If no <b> tag, keep full text

            # Preserve original structure
            if tag_name == "h1":
                formatted_content += f'\n<h1>{formatted_text}</h1>'
            elif tag_name == "h3":
                formatted_content += f'\n<h3>{formatted_text}</h3>'
            elif tag_name == "center":
                formatted_content += f'\n<center>{formatted_text}</center>'
            elif tag_name == "p":
                formatted_content += f'\n<p>{formatted_text}</p>'
            elif tag_name == "div":
                formatted_content += f'\n<div>{formatted_text}</div>'

        chapter_content += formatted_content
        chapter_content += "\n</div>\n</body>"

        # Save XML file
        os.makedirs(Collection_path, exist_ok=True)
        xml_filename = os.path.join(Collection_path, f"{title_name}_{chapter_name}_{section_name}.xml")

        with open(xml_filename, 'w', encoding='utf-8') as f:
            f.write(chapter_content)

        logger.info(f"Saved section in {xml_filename}")
        section_data[section_name] = xml_filename

    return section_data
